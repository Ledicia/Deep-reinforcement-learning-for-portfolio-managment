{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market environment \n",
    "\n",
    "Trading environment (render) of the project:\n",
    "* The trading agent calls the class by giving an action at the time t. \n",
    "* Then the class (render) gives back the new portfolio at the next step (time t+1). \n",
    "\n",
    "Parameters:\n",
    "* windonw_length: Number of time slots looked in the past to build the input tensor.\n",
    "* portfolio_value: Initial value of the portfolio.\n",
    "* trading_cost: Cost (in % of the traded stocks) the agent will pay to execute the action.\n",
    "* interest_rate: Rate of interest (in % of the money the agent has) the agent will:\n",
    "                  -get at each step if he has a positive amount of money \n",
    "                  -pay if he has a negative amount of money\n",
    "* train_size: % of data taken for the training of the agent - please note the training data are taken with respect \n",
    "    of the time span (train -> | time T | -> test)\n",
    "    \n",
    "Calculations for each of the samples (not batches): \n",
    "The portfolio features are the tensors that characterize the portfolio:\n",
    "\n",
    "* Relative price vector ($y_t$): Changes of the prices during the session or relative prices. There are 2 approaches:\n",
    "    - $y_t = \\frac{\\text{closing}_t}{\\text{opening}_{t}}$. Shape $[1, 1+m]$ vector of relative prices for each asset and 1 sample.\n",
    "    - $y_t = \\frac{\\text{opening}_t}{\\text{opening}_{t-1}}$\n",
    "\n",
    "* Future_weight_vec ($w'_t$ or $w_{evol}$}): is the portfolio weight vector at the end of the trading period. It is given by:\n",
    "$$w'_t = \\frac{\\vec{y}_t\\vec{w}_{t-1}}{\\sum_{i=1}^m y_{t,i}\\cdot w_{t-1,i}};\\; \\mathrm{Shape}\\; [Batches, 1+m]$$\n",
    "\n",
    "\n",
    "## Simple return:\n",
    "Notation:\n",
    "* $Pf = Pf_{previous}\\cdot r_t = \\sum_{i=1}^{m}\\vec{v}_{previous}$ : portfolio value\n",
    "* $\\vec{v}_{previous} = \\vec{w}\\cdot Pf_{previous}$ : vector of the values of the assets in the portfolio where $\\vec{w}$ is the action just taken.\n",
    "    \n",
    "Given an action, $\\vec{a} = \\vec{w}$, the portfolio value is given by:\n",
    "$$P_t = P_{t-1}\\vec{a} = P_{t-1}\\cdot y_t\\cdot \\vec{a}_{t-1}$$\n",
    "where $y_t =[(1+\\text{int rate}), \\frac{\\text{opening}_t}{\\text{opening}_{t-1}}]$ represents the relative price change. \n",
    "\n",
    "Considering transaction costs (each transaction costs a \"transaction cost\" so the whole operation in the portfolio costs c such that $pf_{evol} = \\mu_t\\cdot pf_{previous}$), the reward of the portfolio is then given by:\n",
    "$$\\text{reward} = \\frac{pf_{evol}-pf_{previous}}{pf_{previous}} = \\frac{\\mu_t P_t}{P_{t-1}}-1 = \\mu_t \\cdot y_t\\cdot w_{t-1}-1$$ \n",
    "where $pf_{evol}$ is the change in total portfolio value (the change is measured by $\\vec{y}_t$) once transaction costs have been considered:\n",
    "\n",
    "$pf_{evol} = \\sum_{i=0}^{m} \\vec{v}_{evol} =  \\sum_{i=0}^{m} \\vec{v}_{trans}\\cdot \\vec{y}_t$ and $\\vec{v}_{trans} = \\vec{v}_{previous} - (costs, 0,0,\\dots , 0)$\n",
    "\n",
    "\n",
    "<!--- $pf_{evol} = (pf_{previous} - \\text{costs})\\cdot y_t = \\mu_t \\cdot w_{alloc} \\cdot y_t = \\mu_t \\cdot w_{t-1} \\cdot y_t $-->\n",
    "\n",
    "Therefore, knowing $\\vec{v}_{evol}$ and $pf_{evol}$, the new action considering transaction costs should be:\n",
    "\n",
    "$$w_{evol} = \\frac{v_{evol}}{\\sum _{i = 1}^{m}v_{evol,i}}$$\n",
    "\n",
    "## Log return:\n",
    "\n",
    "* Logarithmic rate of return ($r_t$) or immediate reward: $\\log{\\mu_t y_t \\cdot w_{t-1}}$. Shape $[1, 1]$\n",
    "\n",
    "* Portfolio value vector (__pv_vector): Portfolio value for each batch (the value of the portfolio after computing the action with n_b samples)\n",
    "    - $[Batch]$ rank 1 tensor (vector):  There is a value per batch.\n",
    "    - Portfolio value ($P_f$): is the value of the portfolio anfter $\\Delta t = t_f-t_0$ periods:\n",
    "$$P_{t_f} = P_0 \\exp \\left( \\sum _{t=1} ^{t_f + 1} r_t \\right) = P_0 \\prod _{t=1} ^{t_f+1} \\mu_t \\vec{y}_t \\cdot \\vec{w}_{t-1}; \\; \\mathrm{Shape}\\; \\mathrm{It\\; is\\; a\\; scalar. Shape\\; []}$$ \n",
    "\n",
    "* Cumulative reward function ($R$): is what is going to be maximize. It is given by the average of logarithmic cumulated return\n",
    "$$R(s_1, a_1, \\dots, s_{t_f}, a_{t_f}, s_{t_f+1}) = \\frac{1}{t_f}\\log \\left(\\frac{P_f}{P_0}\\right) = \\sum _{t=1}^{t_f+1}\\log (\\mu_t\\vec{y}_t\\cdot \\vec{w}_{t-1}) = \\frac{1}{t_f}\\sum_{t=1}^{t_f+1}r_t; \\; \\mathrm{Shape}\\; [Batches,1]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "# from gym.utils import seeding\n",
    "import numpy as np\n",
    "from gym.envs.registration import register\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class MarketEnvironment():\n",
    "\n",
    "    def __init__(self, path, window_length, initial_portfolio_value, \n",
    "                 trading_cost, interest_rate, train_size, LogReturn):\n",
    "        \n",
    "        # Load data [features, assets, previous periods]\n",
    "        self.path = path\n",
    "        self.data = np.load(self.path)\n",
    "\n",
    "        # Parameters of the trading agent\n",
    "        self.portfolio_value = initial_portfolio_value  # How much cash used to create the portfolio\n",
    "        self.window_length = window_length\n",
    "        self.trading_cost = trading_cost\n",
    "        self.interest_rate = interest_rate\n",
    "        self.LogReturn = LogReturn\n",
    "\n",
    "        # Number of stocks and features\n",
    "        self.num_stocks = self.data.shape[1]\n",
    "        self.num_features = self.data.shape[0]\n",
    "        self.end_train = int((self.data.shape[2]-self.window_length)*train_size)\n",
    "        \n",
    "        # Init state and index\n",
    "        self.index = None  # Represents the period t for which the computations are made\n",
    "        self.state = None\n",
    "        self.done = False\n",
    "\n",
    "        #init seed\n",
    "#         self.seed()\n",
    "        \n",
    "    # Return the value of the portfolio\n",
    "    def return_pf(self):\n",
    "        return self.portfolio_value\n",
    "        \n",
    "    # Tensor which will be the input for the NN [Batches, features, assets, previous periods]   \n",
    "    def readTensor(self, X, index):\n",
    "        # Reads index-n:index -> t + n - n: t + n \n",
    "        # index = t + n where t is the period given\n",
    "        return X[ : , :, index-self.window_length:index]\n",
    "    \n",
    "    # Calculate the return of each stock for the day t \n",
    "    def getFluctuationVector(self, index):\n",
    "        #print(index)\n",
    "        # Adds the element [1+self.interest_rate] to the first possiton of the data[-1,:,index] (last feature, all assets, period t + n = index)\n",
    "        return np.array([1 + self.interest_rate] + self.data[-1,:,index].tolist())\n",
    "\n",
    "    # Get random seed\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    # Restarts the environment with given initial weights and given value of portfolio\n",
    "    def reset(self, w_init, p_init, t = 0):\n",
    "        \n",
    "        self.index = self.window_length + t  # Period for which the computations are made\n",
    "        self.state= (self.readTensor(self.data, self.index), w_init, p_init)\n",
    "        self.done = False\n",
    "        \n",
    "        return self.state, self.done\n",
    "    \n",
    "\n",
    "    # Compute the new value of the portfolio\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        At each step t, the trading agent gives as input the action he wants to do: w_t\n",
    "        The function computes the new value of the portfolio at the step (t+1): P_{t+1}\n",
    "        It also returns the reward associated with the action the agent took: r_t \n",
    "        The reward is defined as the evolution of the the value of the portfolio in %. \n",
    "        \"\"\"\n",
    "        index = self.index\n",
    "        # Get tensor X (self.data), from index-window_size:index previous periods\n",
    "        data = self.readTensor(self.data, index)\n",
    "        done = self.done\n",
    "\n",
    "        # Beginning of the day (t)\n",
    "        state = self.state      # State of the market {X_t, w_{t-1}, pf_{t-1}}\n",
    "        w_previous = state[1]   # Action taken in period t-1 evolved due to fluctuations of price during (t-1, t) session\n",
    "        pf_previous = state[2]  # Value of the portfolio at the end of the previous period (t-1)\n",
    "        \n",
    "        # Fluctuations in price plus the interest rate for money (to evaluate cash bias): [1+int_rate, open(index+1)/open(index)]\n",
    "        update_vector = self.getFluctuationVector(index)\n",
    "            \n",
    "        # Compute transaction cost\n",
    "        cost = pf_previous * np.linalg.norm((action[1:] - w_previous[1:]), ord = 1) * self.trading_cost\n",
    "        \n",
    "        # Transaction remainder factor: Pevol = mu*Pprev = Pprev - costs; mu = 1-costs\n",
    "        # So as to calculate the reward without the need of the portfolio previous value\n",
    "#         mu = 1 - np.linalg.norm((action-w_previous), ord = 1)* self.trading_cost\n",
    "\n",
    "        # Value vector of the assets: value of each of the assets in the portfolio for the new action\n",
    "        v_alloc = pf_previous * action\n",
    "        \n",
    "        # Pay transaction costs (pay the from cash amount)\n",
    "        v_trans = v_alloc - np.array([cost] + [0] * self.num_stocks)\n",
    "        \n",
    "        # Compute features of period t considering the price evolution during the session\n",
    "        v_evol = v_trans * update_vector  # Value of each asset at the end of the t period\n",
    "        pf_evol = np.sum(v_evol)        # Portfolio value at the end of period t\n",
    "        w_evol = v_evol / pf_evol         # Weight vector at the end of period t\n",
    "        \n",
    "        # Compute reward:\n",
    "        if not self.LogReturn:\n",
    "            reward = (pf_evol - pf_previous)/pf_previous\n",
    "#             reward2 = np.dot(action, update_vector)*mu - 1     # Same result\n",
    "        else:\n",
    "            reward = np.log(pf_evol/pf_previous)\n",
    "#             reward2 = np.log(np.dot(action,update_vector)*mu)  # Same result\n",
    "           \n",
    "        # Update index to get the next state (state of the following sample)\n",
    "        index = index + 1  \n",
    "        state = (self.readTensor(self.data, index), w_evol, pf_evol)  # State = (X_t+1, w't, p't)\n",
    "        \n",
    "        if index >= self.end_train:\n",
    "            done = True\n",
    "    \n",
    "        self.state = state\n",
    "        self.index = index\n",
    "        self.done = done\n",
    "\n",
    "        return state, reward, done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
