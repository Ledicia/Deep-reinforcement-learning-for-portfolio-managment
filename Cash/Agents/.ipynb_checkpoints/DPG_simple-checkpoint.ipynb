{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "class DPG_S:\n",
    "    \n",
    "    def __init__(self, feature_number, num_assets, window_size, sess, optimizer,\n",
    "                 trading_cost, interest_rate, LogReturn = False, layer_type = 'Conv'):\n",
    "\n",
    "        # parameters\n",
    "        self.trading_cost = trading_cost\n",
    "        self.interest_rate = interest_rate\n",
    "        self.num_features = feature_number\n",
    "        self.n = window_size\n",
    "        self.m = num_assets\n",
    "        self.LogReturn = LogReturn\n",
    "        self.layer_type = layer_type\n",
    "        \n",
    "        # Tensor of the prices\n",
    "        self.X_t = tf.placeholder(tf.float32, [None, self.num_features, self.m, self.n]) \n",
    "        self.batch_size = tf.shape(self.X_t)[0]                         # Batch size\n",
    "        self.W_previous = tf.placeholder(tf.float32, [None, self.m+1])  # w'_{t-1}\n",
    "        self.action = self.build_net()  # Returns the output of imputing X_t and w_previous to the NN\n",
    "        \n",
    "        self.pf_value_previous = tf.placeholder(tf.float32, [None, 1])   # p'_{t-1} \n",
    "        self.dailyReturn_t = tf.placeholder(tf.float32, [None, self.m])  # y_t = Open(t+1)/Open(t)\n",
    "        constant_return = tf.constant(1+self.interest_rate, shape=[1, 1])            # Interest rate given by cash\n",
    "        self.cash_return = tf.tile(constant_return, tf.stack([self.batch_size, 1]))  # Interest rate is the sae for all samples\n",
    "        self.y_t = tf.concat([self.cash_return, self.dailyReturn_t], axis=1)         # Daily returns considering the cash return\n",
    "        self.loss_function = self.loss_function()\n",
    "        \n",
    "        # Objective function: maximize reward over the batch (min(-r) = max(r))\n",
    "        self.train_op = optimizer.minimize(-self.loss_function)\n",
    "        self.optimizer = optimizer\n",
    "        self.sess = sess\n",
    "        \n",
    "        \n",
    "    def build_net(self):\n",
    "        state = tf.transpose(self.X_t, [0, 2, 3, 1])  # Reshape [Batches, Assets, Periods, Features]\n",
    "        if self.layer_type == 'Conv':\n",
    "            network = tflearn.layers.conv_2d(state, 2,\n",
    "                                             [1, 2],\n",
    "                                             [1, 1, 1, 1],\n",
    "                                             'valid',\n",
    "                                             'relu')\n",
    "        else:\n",
    "            resultlist = []\n",
    "            reuse = False\n",
    "            neuron_number = 20\n",
    "            for i in range(state.get_shape()[1]):\n",
    "                if i > 0:\n",
    "                    reuse = True\n",
    "                if self.layer_type == \"LSTM\":\n",
    "                    result = tflearn.layers.lstm(state[:, :, :, i],\n",
    "                                                 neuron_number,                \n",
    "                                                 dropout = 0.6,\n",
    "                                                 scope = \"lstm\",\n",
    "                                                 reuse = reuse)\n",
    "                elif self.layer_type == 'RNN':\n",
    "                    result = tflearn.layers.simple_rnn(network[:, :, :, i],\n",
    "                                                       neuron_number,\n",
    "                                                       dropout = 0.6,\n",
    "                                                       scope=\"rnn\",\n",
    "                                                       reuse=reuse)\n",
    "                resultlist.append(result)\n",
    "            network = tf.stack(resultlist)\n",
    "            network = tf.transpose(network, [1, 0, 2])\n",
    "            network = tf.reshape(network, [-1, state.get_shape()[1], 1, neuron_number]) \n",
    "\n",
    "            \n",
    "            \n",
    "        width = network.get_shape()[2]\n",
    "        network = tflearn.layers.conv_2d(network, 48,\n",
    "                                         [1, width],\n",
    "                                         [1, 1],\n",
    "                                         \"valid\",\n",
    "                                         'relu',\n",
    "                                         regularizer=\"L2\",\n",
    "                                         weight_decay=5e-9)\n",
    "        w_previous = self.W_previous[:, 1:]\n",
    "        network=tf.concat([network,tf.reshape(w_previous, [-1, self.m, 1, 1])],axis=3)\n",
    "        network = tflearn.layers.conv_2d(network, 1,\n",
    "                                         [1, network.get_shape()[2]],\n",
    "                                         [1, 1],\n",
    "                                         \"valid\",\n",
    "                                         'relu',\n",
    "                                         regularizer=\"L2\",\n",
    "                                         weight_decay=5e-9)\n",
    "        network = network[:, :, 0, 0]  # Squeeze diensions [Batchs, assets, 1, 1] = [Batches, Assets]\n",
    "#         with tf.variable_scope(\"cash_bias\", reuse=tf.AUTO_REUSE):\n",
    "#             bias = tf.get_variable(\"cash_bias\", [1, 1], dtype=tf.float32, initializer=tf.zeros_initializer)\n",
    "        if self.LogReturn:\n",
    "            bias = tf.get_variable(\"cash_bias_log\", [1, 1], dtype=tf.float32, initializer=tf.zeros_initializer)\n",
    "        else:\n",
    "            bias = tf.get_variable(\"cash_bias_simple\", [1, 1], dtype=tf.float32, initializer=tf.zeros_initializer)\n",
    "        #bias = tf.get_variable('cash_bias', shape=[1, 1], initializer=tf.constant_initializer(0.7))\n",
    "        cash_bias = tf.tile(bias, tf.stack([self.batch_size, 1]))\n",
    "        network = tf.concat([cash_bias, network], 1)          # concatenates adding cols (the number of rows does not change)\n",
    "        self.voting = network                                 # voting scores\n",
    "        action = tf.nn.softmax(network)\n",
    "#         network=tf.layers.flatten(network)\n",
    "#         w_init = tf.random_uniform_initializer(-0.005, 0.005)\n",
    "#         action = tf.layers.dense(network, self.m, activation=tf.nn.softmax, kernel_initializer=w_init)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def loss_function(self):\n",
    "        if self.LogReturn:\n",
    "\n",
    "            # PROFIT VECTOR: P_t/P_{t-1} = exp(r_t) = sum over the assets (action*y_t) \n",
    "            # profit_vector = (y_t1 * w_t1,..., y_tn * w_tn) tn = t1 + batch_size = last period (sample in the batch)\n",
    "            self.profit_vector = tf.reduce_sum(self.action * self.y_t, reduction_indices=[1]) * self.compute_mu() \n",
    "\n",
    "            # PROFIT: P(t)/P(t-bs)=exp(sum(_(t-bs)^t) r_t) = (prod(_(t-bs)^t)w_t*y_t) profit obtained after each batch\n",
    "            self.profit = tf.reduce_prod(self.profit_vector)          # ultiplies all the elements of profit_vector\n",
    "            self.mean = tf.reduce_mean(self.profit_vector)            # Mean of the portfolio value vector (through all the batches)\n",
    "            self.reward = tf.reduce_mean(tf.log(self.profit_vector))  # Cumulated return (eq 22)\n",
    "            loss_function = self.set_loss_function()                  # Loss function to train the NN\n",
    "\n",
    "            # Evaluate performance\n",
    "            self.standard_deviation = tf.sqrt(tf.reduce_mean((self.profit_vector - self.mean) ** 2))\n",
    "            self.sharp_ratio = (self.mean - 1) / self.standard_deviation\n",
    "            \n",
    "#             # Another way that seems to work pretty well:\n",
    "#             w_eq = np.array(np.array([1/(self.m+1)]*(self.m+1)))\n",
    "#             profit_vector_eq = tf.reduce_sum(w_eq * self.y_t, reduction_indices=[1])\n",
    "#             profit_eq = tf.reduce_prod(profit_vector_eq) \n",
    "#             max_weight = tf.reduce_max(self.action)\n",
    "#             ratio_regul = 0.1\n",
    "#             loss_function = self.profit - profit_eq - ratio_regul * max_weight\n",
    "            \n",
    "\n",
    "        # Simple reward: r_t = (p_t-p_t-1)/p_t-1 = mu_t*y_t*w_t - 1 (w_t = action)\n",
    "        else:   \n",
    "            # Vector of the returns obtained for each period (r_t1, ..., r_tn) such that tn = t1+batch_size\n",
    "            self.profit_vector = tf.reduce_sum(self.action * self.y_t, reduction_indices=[1]) * self.compute_mu() - 1\n",
    "            \n",
    "            # r_t = (p_t-p_t-1)/p_t-1 => p_t/p_t-1 = r_t + 1 => p_t = p_t-1(r_1 + 1)\n",
    "            self.profit = tf.reduce_prod((1 + self.profit_vector))\n",
    "            self.mean = tf.reduce_mean(self.profit_vector)          # Mean of the portfolio value vector (through all the batches)\n",
    "            self.reward = tf.reduce_mean(self.profit)               # Cumulated return (eq 22)\n",
    "            loss_function =  self.set_loss_function()               # Loss function to train the NN\n",
    "            #loss_function =  tf.reduce_mean(self.profit_vector)     # Loss function to train the NN\n",
    "\n",
    "            ## Evaluate performance\n",
    "#             self.standard_deviation = tf.sqrt(tf.reduce_mean((self.profit_vector - self.mean) ** 2))\n",
    "#             self.sharp_ratio = (self.mean - 1) / self.standard_deviation\n",
    "            \n",
    "#             # Another way that seems to work pretty well:\n",
    "#             Vprime_t = self.action * self.pf_value_previous     # Asset values at the end of the period t before reallocating [Batches, assets]\n",
    "#             Vprevious = self.W_previous*self.pf_value_previous  # Asset values at the beggining of the period [Batches, assets]\n",
    "\n",
    "#             cost = self.trading_cost * tf.norm(Vprime_t-Vprevious, ord=1, axis=1) # Rank 1 [Batch]: one cost per sample (cost_t1,..., cost_tn)\n",
    "#             cost = tf.expand_dims(cost, 1)                                        # One cost per sample (Rank 2) [Batch, 1]\n",
    "\n",
    "#             # Create a cost vector for each sample: (cost_sample, 0, 0, ..., 0) \n",
    "#             # The batch cost tensor is composed for each of the sample vectors\n",
    "#             zero = tf.constant(np.array([0.0]*self.m).reshape(1, self.m), shape=[1, self.m], dtype=tf.float32)\n",
    "#             vec_zero = tf.tile(zero, tf.stack([self.batch_size, 1]))  # [Batches, Non cash assets]\n",
    "#             vec_cost = tf.concat([cost, vec_zero], axis=1)            # [Batches, Non cash assets + cash] = [Batches, 1+m]\n",
    "\n",
    "#             Vsecond_t = Vprime_t - vec_cost\n",
    "\n",
    "#             V_t = tf.multiply(Vsecond_t, self.y_t)     # [Batches, 1+m] one vector of monetary value of assets per sample\n",
    "#             self.portfolioValue = tf.norm(V_t, ord=1)  # Sum of V_t for each sample (value for each period) and for each batch\n",
    "#             self.instantaneous_reward = (self.portfolioValue-self.pf_value_previous)/self.pf_value_previous  # simple return\n",
    "            \n",
    "\n",
    "#             # Calculate the reward obtained for an equiweighted portfolio so as to build the loss function\n",
    "#             w_eq = np.array(np.array([1/(self.m+1)]*(self.m+1)))\n",
    "#             V_eq = w_eq*self.pf_value_previous\n",
    "#             V_eq_second = tf.multiply(V_eq, self.y_t)\n",
    "#             self.portfolioValue_eq = tf.norm(V_eq_second, ord=1)\n",
    "#             self.instantaneous_reward_eq = (self.portfolioValue_eq-self.pf_value_previous)/self.pf_value_previous\n",
    "\n",
    "            # For regularizing loss function\n",
    "            self.max_weight = tf.reduce_max(self.action)\n",
    "            ratio_regul = 0.1\n",
    "            # Minimizes the -loss_function: maxiizes the difference between the reward obtained by the agents action and \n",
    "            # the reward obtained by an agnt which invests same weight in each asset\n",
    "            #loss_function = self.instantaneous_reward - self.instantaneous_reward_eq - ratio_regul*self.max_weight\n",
    "\n",
    "        return loss_function\n",
    "                \n",
    "    \n",
    "    # Transaction remainder factor \n",
    "    def compute_mu(self):\n",
    "        c = self.trading_cost\n",
    "        # Starts in [:,1:] to not consider the cash in the calculations\n",
    "        return 1-tf.reduce_sum(tf.abs(self.action[:,1:]-self.W_previous[:,1:]),axis=1)*c  # [Batches,]\n",
    "   \n",
    "    \n",
    "    # Define the loss function which is going to minimize the agent (so as to maximize the reward)\n",
    "    # Keep in mind that what is going to be minimize is the -loss function (see self.train_op)\n",
    "    def set_loss_function(self):\n",
    "        LAMBDA = 1e-4 \n",
    "        \n",
    "        # Minimizes minus the portfolio value (maximizes the portfolio value)\n",
    "        def loss_function1():\n",
    "            if self.LogReturn:\n",
    "                return tf.reduce_mean(tf.log(self.profit_vector))\n",
    "            else: \n",
    "                return tf.reduce_mean(self.profit_vector)\n",
    "\n",
    "        # Adds regularization\n",
    "        def loss_function2():\n",
    "            if self.LogReturn:\n",
    "                return tf.reduce_mean(tf.log(self.profit_vector)) - \\\n",
    "                   LAMBDA * tf.reduce_mean(tf.reduce_sum(tf.log(1 + 1e-6 + self.action), reduction_indices=[1]))\n",
    "            else: \n",
    "                return tf.reduce_mean(self.profit_vector) - \\\n",
    "                   LAMBDA * tf.reduce_mean(self.profit_vector)\n",
    "\n",
    "        # Mean of the returns obtained minus the amount of money that takes to change the portfolio values \n",
    "        def with_last_w():\n",
    "            if self.LogReturn:\n",
    "                return tf.reduce_mean(tf.log(tf.reduce_sum(self.action[:] * self.y_t, reduction_indices=[1])\n",
    "                                          -tf.reduce_sum(tf.abs(self.action[:, 1:] - self.W_previous[:,1:])\n",
    "                                                         *self.trading_cost, reduction_indices=[1])))\n",
    "            else:\n",
    "                return tf.reduce_mean(self.profit_vector \\\n",
    "                                          - tf.reduce_sum(tf.abs(self.action[:, 1:] - self.W_previous[:,1:])\n",
    "                                                         *self.trading_cost, reduction_indices=[1]))\n",
    "\n",
    "        loss_function = loss_function1\n",
    "        loss_tensor = loss_function()\n",
    "        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        if regularization_losses:\n",
    "            for regularization_loss in regularization_losses:\n",
    "                loss_tensor += regularization_loss\n",
    "        return loss_tensor\n",
    "\n",
    "    # Compute the agent's action   \n",
    "    def compute_W(self, X_t_, W_previous_):\n",
    "        return self.sess.run(tf.squeeze(self.action), feed_dict={self.X_t: X_t_, self.W_previous: W_previous_})\n",
    "    \n",
    "    def get_reward(self,  X_t_, W_previous_, pf_value_previous_, dailyReturn_t_):\n",
    "        return self.sess.run(self.loss_function, feed_dict={self.X_t: X_t_,                             \n",
    "                                                self.W_previous: W_previous_,\n",
    "                                                self.pf_value_previous: pf_value_previous_,\n",
    "                                                self.dailyReturn_t: dailyReturn_t_})\n",
    "\n",
    "    # Train the NN maximizing the reward: the input is a batch of the differents values\n",
    "    def train(self, X_t_, W_previous_, pf_value_previous_, dailyReturn_t_):\n",
    "     \n",
    "        self.sess.run(self.train_op, feed_dict={self.X_t: X_t_,                             \n",
    "                                                self.W_previous: W_previous_,\n",
    "                                                self.pf_value_previous: pf_value_previous_,\n",
    "                                                self.dailyReturn_t: dailyReturn_t_})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
