{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function, division\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pgportfolio.constants import *\n",
    "import pgportfolio.learn.network as network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Trading Agent\n",
    "Class to implement a policy gradient trading agent. This agent is in charge of finding a policy which maximizes the mean of the cumulative reward function of the portfolio. Since the agent is a NN, the CNN class defined in NeuralNetwor.ipynb is going to be instanciated in order to create the agent. Then, by calling the method _build_network, the output of the NN (weight vector $\\vec{w}$ or action) is computed by the layers specified in a json file which is fedin the constructor of the agent.\n",
    "\n",
    "This class computes the portfolio features and trains the NN based on those features.\n",
    "\n",
    "\n",
    "## 1.1 Portolio features:\n",
    "The portfolio features are the tensors that characterize the portfolio:\n",
    "\n",
    "* Relative price tensor $Y_t$: Composed by the relative price vectors of the 3 features (future_price vector = closing price($v_t$)/opening price($v_{t-1}$)), where the shape is $[Bathces, f, m]$, where f are the features, and m the non cash assets.\n",
    "\n",
    "* Relative price vector ($y_t$): Relative prices of the closing price (feature 0). Shape $[Batches, 1+m]$. It is a rank 2 tensor, and it can be seen as a vector (rank 1 tensor) for each batch ($n_b$ samples/periods).\n",
    "\n",
    "* Future_weight_vec ($w'_t$): is the portfolio weight vector at the end of the trading period. It is given by:\n",
    "$$w'_t = \\frac{\\vec{y}_t\\vec{w}_{t-1}}{\\sum_{i=1}^m y_{t,i}\\cdot w_{t-1,i}};\\; \\mathrm{Shape}\\; [Batches, 1+m]$$\n",
    "\n",
    "* Logarithmic rate of return ($r_t$) or immediate reward: $\\log{\\mu_t y_t \\cdot w_{t-1}}$. Shape $[Batches, 1]$\n",
    "\n",
    "* Portfolio value vector (__pv_vector): Portfolio value for each batch (the value of the portfolio after computing the action with n_b samples)\n",
    "    - $[Batch]$ rank 1 tensor (vector):  There is a value per batch.\n",
    "    - Portfolio value ($P_f$): is the value of the portfolio anfter $\\Delta t = t_f-t_0$ periods:\n",
    "$$P_{t_f} = P_0 \\exp \\left( \\sum _{t=1} ^{t_f + 1} r_t \\right) = P_0 \\prod _{t=1} ^{t_f+1} \\mu_t \\vec{y}_t \\cdot \\vec{w}_{t-1}; \\; \\mathrm{Shape}\\; \\mathrm{It\\; is\\; a\\; scalar. Shape\\; []}$$ \n",
    "\n",
    "* Cumulative reward function ($R$): is what is going to be maximize. It is given by the average of logarithmic cumulated return\n",
    "$$R(s_1, a_1, \\dots, s_{t_f}, a_{t_f}, s_{t_f+1}) = \\frac{1}{t_f}\\log \\left(\\frac{P_f}{P_0}\\right) = \\sum _{t=1}^{t_f+1}\\log (\\mu_t\\vec{y}_t\\cdot \\vec{w}_{t-1}) = \\frac{1}{t_f}\\sum_{t=1}^{t_f+1}r_t; \\; \\mathrm{Shape}\\; [Batches,1]$$\n",
    "\n",
    "\n",
    "## 1.2 Training the trading agent aka NN\n",
    "\n",
    "1. Creation of the agent object which is the instance of the CNN class. \n",
    "2. Fed the batch input tensor, the relative price tensor $Y_t$, the previous weight vector, and the number of batches ($N_b$) into the NN. \n",
    "3. The policy network will be trained against $N_b$ randomly chosen mini-batches from this set of n previous periods. Each mini-batch contains $n_b$ samples/periods of the data.\n",
    "4. A batch starting with period tb $t_0 âˆ’ n_b$ is picked with a geometrically distributed probability (ReplayBuffer.ipynb).\n",
    "5.  It is important that prices inside a batch are in time-order.\n",
    "6. __set_loss_function: Sets the loss function also called objective function. It is the function that the agent wants to minimize so as to update the parameters. This function is the -reward, cause the agent looks forward maximizing the reward.\n",
    "7. init_train: Define which is going to be the optimizer used to minimize the loss (train_step.)\n",
    "8. train: calls evaluate_tensors, checking that the tensors fed into the NN have no nan value, and trains the NN with the previously defined optimizer.\n",
    "9. decide_by_history: Once the NN outputs the action, this function runs the computational graph defined in the constructor, returning each of its values (features of the portfolio) updated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPG_LogReturn:\n",
    "    def __init__(self, config, restore_dir=None, device=\"cpu\"):\n",
    "        '''\n",
    "        From the CNN object (the class is defined inside Neural Network) outputs the weight vector or action vector.\n",
    "        Calculates the return and the cumulated reward\n",
    "        Defined loss functions based on this cumulated reward we want to maximize.\n",
    "        The Net, after each batch computes the loss and updates parameters.\n",
    "        config: network configuration\n",
    "        '''\n",
    "        self.__config = config  # Read the config file where training parameters are defined\n",
    "        self.__asset_number = config[\"input\"][\"asset_number\"]  # Number of assets to be traded (does not include cash)\n",
    "        \n",
    "        ## Create the CNN object in order to train the net. It outputs the weiht vector depending on config[\"layers\"]\n",
    "        self.__net = network.CNN(config[\"input\"][\"feature_number\"],  # feature number (f)\n",
    "                                 self.__asset_number,                # rows: number of non cash assets in the portfolio (m)\n",
    "                                 config[\"input\"][\"window_size\"],     # cols: number of previous periods FOR A SPECIFIC ASSET (n)\n",
    "                                 config[\"layers\"],                   # dict of layers\n",
    "                                 device=device)\n",
    "        \n",
    "        ## Keep track of global steps during training\n",
    "        self.__global_step = tf.Variable(0, trainable=False)\n",
    "        self.__train_operation = None\n",
    "        \n",
    "        ## Compute portfolio features:\n",
    "        \n",
    "        # FUTURE RELATIVE PRICE RANK 3 TENSOR Y: tensor for the last period (t)\n",
    "        self.__Y = tf.placeholder(tf.float32, shape=[None, self.__config[\"input\"][\"feature_number\"], self.__asset_number])\n",
    "        \n",
    "        # FUTURE RELATIVE PRICERANK 2 TENSOR y: relative prices considering just the closing prices (feature 0) at t\n",
    "        self.__y = tf.concat([tf.ones([self.__net.input_num, 1]), self.__Y[:, 0, :]], 1)\n",
    "        \n",
    "        # WEIGHT VECTOR: Computing w' at the end of each batch\n",
    "        self.__future_weight_vec = (self.__y * self.__net.output) / tf.reduce_sum(self.__y * self.__net.output, axis=1)[:, None]\n",
    "                    \n",
    "        # tf.assert_equal(tf.reduce_sum(self.__future_weight_vec, axis=1), tf.constant(1.0))\n",
    "        \n",
    "        # COMISIONS:\n",
    "        # At the end of period t, the agent has to reallocate w'_t (future_weight_vec) into w_t\n",
    "        # The reallocation of the assets has a cost (transaction cost)\n",
    "        self.__commission_ratio = self.__config[\"trading\"][\"trading_consumption\"]\n",
    "        \n",
    "        # PORTFOLIO VALUE VECTOR:\n",
    "        # __pv_vector: portfolio value for each batch (the value of the portfolio after computing the action with n_b samples)\n",
    "        # __pure_pc: function defined below which returns mu such that w_t = w'_t*mu,  rank 0 tensor (scalar)\n",
    "        # The operations are computed over all the periods in a batch \n",
    "        self.__pv_vector = tf.reduce_sum(self.__net.output * self.__y, reduction_indices=[1]) *\\\n",
    "                           (tf.concat([tf.ones(1), self.__pure_pc()], axis=0))\n",
    "        \n",
    "        # LOG MEAN FREE: cumulated return without considering transaction costs \n",
    "        # dot product of weigght vector of the batches and the future price of the batches\n",
    "        # The dimension is reduced along the asset axis (1), obtaining one value for each batch\n",
    "        self.__log_mean_free = tf.reduce_mean(tf.log(tf.reduce_sum(self.__net.output * self.__y, reduction_indices=[1])))\n",
    "        \n",
    "        # PORTFOLIO VALUE: \n",
    "        # __portfolio_value: result of multiplying all the elements of __pv_vector (values for each batch)\n",
    "        # where each element is the portfolio value for a batch\n",
    "        self.__portfolio_value = tf.reduce_prod(self.__pv_vector)\n",
    "        self.__mean = tf.reduce_mean(self.__pv_vector)              # Mean of the portfolio value vector (through all the batches)\n",
    "        self.__log_mean = tf.reduce_mean(tf.log(self.__pv_vector))  # Cumulated return (eq 22)\n",
    "        \n",
    "        ## Evaluate performance\n",
    "        self.__standard_deviation = tf.sqrt(tf.reduce_mean((self.__pv_vector - self.__mean) ** 2))\n",
    "        self.__sharp_ratio = (self.__mean - 1) / self.__standard_deviation\n",
    "        \n",
    "        ## Train the NN\n",
    "        self.__loss = self.__set_loss_function()\n",
    "        self.__train_operation = self.init_train(learning_rate=self.__config[\"training\"][\"learning_rate\"],\n",
    "                                                 decay_steps=self.__config[\"training\"][\"decay_steps\"],\n",
    "                                                 decay_rate=self.__config[\"training\"][\"decay_rate\"],\n",
    "                                                 training_method=self.__config[\"training\"][\"training_method\"])\n",
    "        self.__saver = tf.train.Saver()\n",
    "        if restore_dir:\n",
    "            self.__saver.restore(self.__net.session, restore_dir)\n",
    "        else:\n",
    "            self.__net.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    ## Getters \n",
    "    @property\n",
    "    def session(self):\n",
    "        return self.__net.session\n",
    "\n",
    "    @property\n",
    "    def pv_vector(self):\n",
    "        return self.__pv_vector\n",
    "\n",
    "    @property\n",
    "    def standard_deviation(self):\n",
    "        return self.__standard_deviation\n",
    "\n",
    "    @property\n",
    "    def portfolio_weights(self):\n",
    "        return self.__net.output\n",
    "\n",
    "    @property\n",
    "    def sharp_ratio(self):\n",
    "        return self.__sharp_ratio\n",
    "\n",
    "    @property\n",
    "    def log_mean(self):\n",
    "        return self.__log_mean\n",
    "\n",
    "    @property\n",
    "    def log_mean_free(self):\n",
    "        return self.__log_mean_free\n",
    "\n",
    "    @property\n",
    "    def portfolio_value(self):\n",
    "        return self.__portfolio_value\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self.__loss\n",
    "\n",
    "    @property\n",
    "    def layers_dict(self):\n",
    "        return self.__net.layers_dict\n",
    "\n",
    "    def recycle(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.__net.session.close()\n",
    "\n",
    "        \n",
    "    # Define the loss function which is going to minimize the agent (so as to maximize the reward)\n",
    "    def __set_loss_function(self):\n",
    "        \n",
    "        # Minimizes the minus the cumulated reward (maximizes the reward)\n",
    "        def loss_function4():\n",
    "            return -tf.reduce_mean(tf.log(tf.reduce_sum(self.__net.output[:] * self.__y,\n",
    "                                                        reduction_indices=[1])))\n",
    "        # Adds regularization\n",
    "        def loss_function5():\n",
    "            return -tf.reduce_mean(tf.log(tf.reduce_sum(self.__net.output * self.__y, reduction_indices=[1]))) + \\\n",
    "                   LAMBDA * tf.reduce_mean(tf.reduce_sum(-tf.log(1 + 1e-6 - self.__net.output), reduction_indices=[1]))\n",
    "\n",
    "        # Minimizes minus the portfolio value (maximizes the portfolio value)\n",
    "        def loss_function6():\n",
    "            return -tf.reduce_mean(tf.log(self.pv_vector))\n",
    "\n",
    "        # Adds regularization\n",
    "        def loss_function7():\n",
    "            return -tf.reduce_mean(tf.log(self.pv_vector)) + \\\n",
    "                   LAMBDA * tf.reduce_mean(tf.reduce_sum(-tf.log(1 + 1e-6 - self.__net.output), reduction_indices=[1]))\n",
    "\n",
    "        # Considers the differences between previous weight vector and the computed one times comision ratio\n",
    "        def with_last_w():\n",
    "            return -tf.reduce_mean(tf.log(tf.reduce_sum(self.__net.output[:] * self.__y, reduction_indices=[1])\n",
    "                                          -tf.reduce_sum(tf.abs(self.__net.output[:, 1:] - self.__net.previous_w)\n",
    "                                                         *self.__commission_ratio, reduction_indices=[1])))\n",
    "\n",
    "        loss_function = loss_function5\n",
    "        if self.__config[\"training\"][\"loss_function\"] == \"loss_function4\":\n",
    "            loss_function = loss_function4\n",
    "        elif self.__config[\"training\"][\"loss_function\"] == \"loss_function5\":\n",
    "            loss_function = loss_function5\n",
    "        elif self.__config[\"training\"][\"loss_function\"] == \"loss_function6\":\n",
    "            loss_function = loss_function6\n",
    "        elif self.__config[\"training\"][\"loss_function\"] == \"loss_function7\":\n",
    "            loss_function = loss_function7\n",
    "        elif self.__config[\"training\"][\"loss_function\"] == \"loss_function8\":\n",
    "            loss_function = with_last_w\n",
    "\n",
    "        loss_tensor = loss_function()\n",
    "        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        if regularization_losses:\n",
    "            for regularization_loss in regularization_losses:\n",
    "                loss_tensor += regularization_loss\n",
    "        return loss_tensor\n",
    "\n",
    "    \n",
    "    # Define the optimizer operation (train_step)\n",
    "    def init_train(self, learning_rate, decay_steps, decay_rate, training_method):\n",
    "        learning_rate = tf.train.exponential_decay(learning_rate, self.__global_step,\n",
    "                                                   decay_steps, decay_rate, staircase=True)\n",
    "        if training_method == 'GradientDescent':\n",
    "            train_step = tf.train.GradientDescentOptimizer(learning_rate).\\\n",
    "                         minimize(self.__loss, global_step=self.__global_step)\n",
    "        elif training_method == 'Adam':\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate).\\\n",
    "                         minimize(self.__loss, global_step=self.__global_step)\n",
    "        elif training_method == 'RMSProp':\n",
    "            train_step = tf.train.RMSPropOptimizer(learning_rate).\\\n",
    "                         minimize(self.__loss, global_step=self.__global_step)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        return train_step\n",
    "\n",
    "    # Train the NN using the previously defined train_operation\n",
    "    def train(self, x, y, last_w, setw):\n",
    "        tflearn.is_training(True, self.__net.session)\n",
    "        self.evaluate_tensors(x, y, last_w, setw, [self.__train_operation])\n",
    "\n",
    "        \n",
    "    # Before feeding the tensor into the NN, checks that there is no nan values\n",
    "    def evaluate_tensors(self, x, y, last_w, setw, tensors):\n",
    "        \"\"\"\n",
    "        :param x: input tensor\n",
    "        :param y: relative price rank 2 tensor\n",
    "        :param last_w: previous weight to consider transaction costs\n",
    "        :param setw: a function, pass the output w to it to fill the PVM\n",
    "        :param tensors: other tensors so as to update its values when running the training (see TrainTrader.ipynb)\n",
    "        :return: The result of runing the NN (weight vector)\n",
    "        \"\"\"\n",
    "        tensors = list(tensors)\n",
    "        tensors.append(self.__net.output)\n",
    "        assert not np.any(np.isnan(x))\n",
    "        assert not np.any(np.isnan(y))\n",
    "        assert not np.any(np.isnan(last_w)), \"the last_w is {}\".format(last_w)\n",
    "        # The session which is ran by this function is the session of the NN class, so it computes the operations defined\n",
    "        # in that class which are related to the tensors given in the feed_dict. In this case, it returns the weight vector\n",
    "        # computed using the parameter tensors which has to be the train_step.\n",
    "        results = self.__net.session.run(tensors, feed_dict={\n",
    "                                         self.__net.input_tensor: x,          # Enters the net\n",
    "                                          self.__Y: y,\n",
    "                                          self.__net.previous_w: last_w,      # Input of EIIE_Output_WithW together with x\n",
    "                                          self.__net.input_num: x.shape[0]})  # Num batches in the dataset\n",
    "        \n",
    "        # TODO: por que el ultimo elemento. No te devuelve como ultimo elemento el num de batches?\n",
    "        # For all the batches, all the weights in the weight vector (last element in the reward list) but the first one\n",
    "        # (weight associated with cash) is stored in the PVM once it has been computed by the NN.\n",
    "        setw(results[-1][:, 1:]) # Sets all the weiht vectors computed for all the samples in the batch without the cash component\n",
    "        return results[:-1]\n",
    "\n",
    "    \n",
    "    # Save the variables path including file name\n",
    "    def save_model(self, path):\n",
    "        self.__saver.save(self.__net.session, path)\n",
    "\n",
    "        \n",
    "    # Consumption vector (on each periods)\n",
    "    def __pure_pc(self):\n",
    "        c = self.__commission_ratio\n",
    "        # Transaction costs are given by equation 16 in the paper \n",
    "        # Time order: w_prime_t -> action this has transaction costs given by the transaction remainder factor\n",
    "        w_t = self.__future_weight_vec[:self.input_num-1]  # rebalanced (all the weights but the one from last batch)\n",
    "        w_t1 = self.__net.action[1:self.input_num]         # actions for all the batches but the first one \n",
    "        mu = 1 - tf.reduce_sum(tf.abs(w_t1[:, 1:]-w_t[:, 1:]), axis=1)*c  # Equations 15, 16, 17 of the paper\n",
    "        \"\"\"\n",
    "        mu = 1-3*c+c**2\n",
    "\n",
    "        def recurse(mu0):\n",
    "            factor1 = 1/(1 - c*w_t1[:, 0])\n",
    "            if isinstance(mu0, float):\n",
    "                mu0 = mu0\n",
    "            else:\n",
    "                mu0 = mu0[:, None]\n",
    "            factor2 = 1 - c*w_t[:, 0] - (2*c - c**2)*tf.reduce_sum(\n",
    "                tf.nn.relu(w_t[:, 1:] - mu0 * w_t1[:, 1:]), axis=1)\n",
    "            return factor1*factor2\n",
    "\n",
    "        for i in range(20):\n",
    "            mu = recurse(mu)\n",
    "        \"\"\"\n",
    "        return mu\n",
    "\n",
    "    \n",
    "    # The history is a 3d matrix (tensor of rank 4). It returns an asset vector in which the agent should invert \n",
    "    def decide_by_history(self, history, last_w):\n",
    "        assert isinstance(history, np.ndarray),\\\n",
    "            \"the history should be a numpy array, not %s\" % type(history)\n",
    "        assert not np.any(np.isnan(last_w))\n",
    "        assert not np.any(np.isnan(history))\n",
    "        tflearn.is_training(False, self.session)\n",
    "        history = history[np.newaxis, :, :, :]\n",
    "        # The session which is ran by this function is the session of this class, so computes the operations defined\n",
    "        # in this class which are related to the tensors given in the feed_dict\n",
    "        return np.squeeze(self.session.run(self.__net.output, feed_dict={self.__net.input_tensor: history,\n",
    "                                                                         self.__net.previous_w: last_w[np.newaxis, 1:],\n",
    "                                                                         self.__net.input_num: 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Trading Agent evaluating simple return\n",
    "The differences with the previous trading agent are:\n",
    "* This one uses the simple return instead of the logarithic one.\n",
    "* This one uses as the change in the prices $\\frac{\\text{Open}_t}{\\text{Open}_{t-1}}$ so the chane in the portfolio value $P_t$ is given by:\n",
    "$$P_t = P_{t-1} \\cdot \\vec{y}_t\\cdot \\vec{w}_{t-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPG_SimpleReturn(object):  \n",
    "    def __init__(self, config, restore_dir=None, device=\"cpu\"):\n",
    "\n",
    "        self.__config = config  # Read the config file where training parameters are defined\n",
    "        self.__asset_number = config[\"input\"][\"asset_number\"]  # Number of assets to be traded (does not include cash)\n",
    "        \n",
    "        ## Create the CNN object in order to train the net. It outputs the weiht vector depending on config[\"layers\"]\n",
    "        self.__net = network.CNN(config[\"input\"][\"feature_number\"],  # feature number (f)\n",
    "                                 self.__asset_number,                # rows: number of non cash assets in the portfolio (m)\n",
    "                                 config[\"input\"][\"window_size\"],     # cols: number of previous periods FOR A SPECIFIC ASSET (n)\n",
    "                                 config[\"layers\"],                   # dict of layers\n",
    "                                 device=device)\n",
    "        \n",
    "          # parameters\n",
    "        self.trading_cost = trading_cost\n",
    "        self.interest_rate = interest_rate        \n",
    "                \n",
    "        self.constant_return = tf.constant(1+self.interest_rate, shape=[1, 1])\n",
    "        self.cash_return = tf.tile(constant_return, tf.stack([shape_X_t, 1]))\n",
    "        \n",
    "        # variable of the cash bias\n",
    "        bias = tf.get_variable('cash_bias', shape=[1, 1, 1, 1], initializer=tf.constant_initializer(cash_bias_init))\n",
    "        # shape of the tensor == batchsize\n",
    "        shape_X_t = tf.shape(self.X_t)[0]\n",
    "        # trick to get a \"tensor size\" for the cash bias\n",
    "        self.cash_bias = tf.tile(bias, tf.stack([shape_X_t, 1, 1, 1]))\n",
    "        \n",
    "        self.__commission_ratio = self.__config[\"trading\"][\"trading_consumption\"]\n",
    "        \n",
    "         # portfolio value at the previous time step\n",
    "        self.pf_value_previous = tf.placeholder(tf.float32, [None, 1])\n",
    "        # vector of Open(t)/Open(t-1)\n",
    "        self.dailyReturn_t = tf.placeholder(tf.float32, [None, self.m])  # Relative price tensor for each feature\n",
    "        \n",
    "        def reward\n",
    "\n",
    "        y_t = tf.concat([cash_return, self.dailyReturn_t], axis=1)       # Relative price tensor for each feature\n",
    "\n",
    "        Vprime_t = self.action * self.pf_value_previous     # Value at the end of the period  t before reallocating\n",
    "        Vprevious = self.W_previous*self.pf_value_previous  # Value at the beggining of the period\n",
    "\n",
    "        constant = tf.constant(1.0, shape=[1])\n",
    "\n",
    "        cost = self.trading_cost * tf.norm(Vprime_t-Vprevious, ord=1, axis=1)*constant\n",
    "        cost = tf.expand_dims(cost, 1)\n",
    "\n",
    "        zero = tf.constant(np.array([0.0]*m).reshape(1, m), shape=[1, m], dtype=tf.float32)\n",
    "\n",
    "        vec_zero = tf.tile(zero, tf.stack([shape_X_t, 1]))\n",
    "        vec_cost = tf.concat([cost, vec_zero], axis=1)\n",
    "\n",
    "        Vsecond_t = Vprime_t - vec_cost\n",
    "\n",
    "        V_t = tf.multiply(Vsecond_t, y_t)\n",
    "        # Compute the portfolio value\n",
    "        self.portfolioValue = tf.norm(V_t, ord=1)\n",
    "        # Simple return, not logarithmic one\n",
    "        self.instantaneous_reward = (self.portfolioValue-self.pf_value_previous)/self.pf_value_previous\n",
    "        \n",
    "        ## Train the NN\n",
    "        self.__loss = self.__set_loss_function()\n",
    "        \n",
    "        \n",
    "        def rewardEquiweighted(self):\n",
    "            y_t = tf.concat([self.cash_return, self.dailyReturn_t], axis=1)\n",
    "            \n",
    "            w_eq = np.array(np.array([1/(self.__asset_number+1)]*(self.__asset_number+1)))\n",
    "            V_eq = w_eq*self.pf_value_previous\n",
    "            V_eq_second = tf.multiply(V_eq, y_t)\n",
    "\n",
    "            self.portfolioValue_eq = tf.norm(V_eq_second, ord=1)\n",
    "\n",
    "            self.instantaneous_reward_eq = (self.portfolioValue_eq-self.pf_value_previous)/self.pf_value_previous\n",
    "            \n",
    "            return self.instantaneous_reward_eq\n",
    "        \n",
    "        def __set_loss_function():\n",
    "            \n",
    "\n",
    "                \n",
    "            with tf.variable_scope(\"Reward_Equiweighted\"):\n",
    "                constant_return = tf.constant(\n",
    "                    1+self.interest_rate, shape=[1, 1])\n",
    "                cash_return = tf.tile(\n",
    "                    constant_return, tf.stack([shape_X_t, 1]))\n",
    "                y_t = tf.concat(\n",
    "                    [cash_return, self.dailyReturn_t], axis=1)\n",
    "  \n",
    "\n",
    "                V_eq = w_eq*self.pf_value_previous\n",
    "                V_eq_second = tf.multiply(V_eq, y_t)\n",
    "        \n",
    "                self.portfolioValue_eq = tf.norm(V_eq_second, ord=1)\n",
    "            \n",
    "                self.instantaneous_reward_eq = (\n",
    "                    self.portfolioValue_eq-self.pf_value_previous)/self.pf_value_previous\n",
    "                \n",
    "            with tf.variable_scope(\"Max_weight\"):\n",
    "                self.max_weight = tf.reduce_max(self.action)\n",
    "                print(self.max_weight.shape)\n",
    "\n",
    "                \n",
    "            with tf.variable_scope(\"Reward_adjusted\"):\n",
    "                \n",
    "                self.adjested_reward = self.instantaneous_reward - self.instantaneous_reward_eq - ratio_regul*self.max_weight\n",
    "                \n",
    "        #objective function \n",
    "        #maximize reward over the batch \n",
    "        # min(-r) = max(r)\n",
    "        self.train_op = optimizer.minimize(-self.adjested_reward)\n",
    "        \n",
    "        # some bookkeeping\n",
    "        self.optimizer = optimizer\n",
    "        self.sess = sess\n",
    "\n",
    "        \n",
    "    def compute_W(self, X_t_, W_previous_):\n",
    "        \"\"\"\n",
    "        This function returns the action the agent takes \n",
    "        given the input tensor and the W_previous\n",
    "        \n",
    "        It is a vector of weight\n",
    "\n",
    "        \"\"\"\n",
    "        return self.sess.run(tf.squeeze(self.action), feed_dict={self.X_t: X_t_, self.W_previous: W_previous_})\n",
    "\n",
    "    \n",
    "    def train(self, X_t_, W_previous_, pf_value_previous_, dailyReturn_t_):\n",
    "        \"\"\"\n",
    "        This function trains the neural network\n",
    "        maximizing the reward \n",
    "        the input is a batch of the differents values\n",
    "        \"\"\"\n",
    "        self.sess.run(self.train_op, feed_dict={self.X_t: X_t_,\n",
    "                                                self.W_previous: W_previous_,\n",
    "                                                self.pf_value_previous: pf_value_previous_,\n",
    "                                                self.dailyReturn_t: dailyReturn_t_})\n",
    "        \n",
    "        \n",
    "        ## Keep track of global steps during training\n",
    "        self.__global_step = tf.Variable(0, trainable=False)\n",
    "        self.__train_operation = None\n",
    "        \n",
    "        ## Compute portfolio features:\n",
    "        \n",
    "        # FUTURE RELATIVE PRICE RANK 3 TENSOR Y: tensor for the last period (t)\n",
    "        self.__Y = tf.placeholder(tf.float32, shape=[None, self.__config[\"input\"][\"feature_number\"], self.__asset_number])\n",
    "        \n",
    "        # FUTURE RELATIVE PRICERANK 2 TENSOR y: relative prices considering just the closing prices (feature 0) at t\n",
    "        self.__y = tf.concat([tf.ones([self.__net.input_num, 1]), self.__Y[:, 0, :]], 1)\n",
    "        \n",
    "        # WEIGHT VECTOR: Computing w' at the end of each batch\n",
    "        self.__future_weight_vec = (self.__y * self.__net.output) / tf.reduce_sum(self.__y * self.__net.output, axis=1)[:, None]\n",
    "                    \n",
    "        # tf.assert_equal(tf.reduce_sum(self.__future_weight_vec, axis=1), tf.constant(1.0))\n",
    "        \n",
    "        # COMISIONS:\n",
    "        # At the end of period t, the agent has to reallocate w'_t (future_weight_vec) into w_t\n",
    "        # The reallocation of the assets has a cost (transaction cost)\n",
    "        self.__commission_ratio = self.__config[\"trading\"][\"trading_consumption\"]\n",
    "        \n",
    "        # PORTFOLIO VALUE VECTOR:\n",
    "        # __pv_vector: portfolio value for each batch (the value of the portfolio after computing the action with n_b samples)\n",
    "        # __pure_pc: function defined below which returns mu such that w_t = w'_t*mu,  rank 0 tensor (scalar)\n",
    "        # The operations are computed over all the periods in a batch \n",
    "        self.__pv_vector = tf.reduce_sum(self.__net.output * self.__y, reduction_indices=[1]) *\\\n",
    "                           (tf.concat([tf.ones(1), self.__pure_pc()], axis=0))\n",
    "        \n",
    "        # LOG MEAN FREE: cumulated return without considering transaction costs \n",
    "        # dot product of weigght vector of the batches and the future price of the batches\n",
    "        # The dimension is reduced along the asset axis (1), obtaining one value for each batch\n",
    "        self.__log_mean_free = tf.reduce_mean(tf.log(tf.reduce_sum(self.__net.output * self.__y, reduction_indices=[1])))\n",
    "        \n",
    "        # PORTFOLIO VALUE: \n",
    "        # __portfolio_value: result of multiplying all the elements of __pv_vector (values for each batch)\n",
    "        # where each element is the portfolio value for a batch\n",
    "        self.__portfolio_value = tf.reduce_prod(self.__pv_vector)\n",
    "        self.__mean = tf.reduce_mean(self.__pv_vector)              # Mean of the portfolio value vector (through all the batches)\n",
    "        self.__log_mean = tf.reduce_mean(tf.log(self.__pv_vector))  # Cumulated return (eq 22)\n",
    "        \n",
    "        ## Evaluate performance\n",
    "        self.__standard_deviation = tf.sqrt(tf.reduce_mean((self.__pv_vector - self.__mean) ** 2))\n",
    "        self.__sharp_ratio = (self.__mean - 1) / self.__standard_deviation\n",
    "        \n",
    "        ## Train the NN\n",
    "        self.__loss = self.__set_loss_function()\n",
    "        self.__train_operation = self.init_train(learning_rate=self.__config[\"training\"][\"learning_rate\"],\n",
    "                                                 decay_steps=self.__config[\"training\"][\"decay_steps\"],\n",
    "                                                 decay_rate=self.__config[\"training\"][\"decay_rate\"],\n",
    "                                                 training_method=self.__config[\"training\"][\"training_method\"])\n",
    "        self.__saver = tf.train.Saver()\n",
    "        if restore_dir:\n",
    "            self.__saver.restore(self.__net.session, restore_dir)\n",
    "        else:\n",
    "            self.__net.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    ## Getters \n",
    "    @property\n",
    "    def session(self):\n",
    "        return self.__net.session\n",
    "\n",
    "    @property\n",
    "    def pv_vector(self):\n",
    "        return self.__pv_vector\n",
    "\n",
    "    @property\n",
    "    def standard_deviation(self):\n",
    "        return self.__standard_deviation\n",
    "\n",
    "    @property\n",
    "    def portfolio_weights(self):\n",
    "        return self.__net.output\n",
    "\n",
    "    @property\n",
    "    def sharp_ratio(self):\n",
    "        return self.__sharp_ratio\n",
    "\n",
    "    @property\n",
    "    def log_mean(self):\n",
    "        return self.__log_mean\n",
    "\n",
    "    @property\n",
    "    def log_mean_free(self):\n",
    "        return self.__log_mean_free\n",
    "\n",
    "    @property\n",
    "    def portfolio_value(self):\n",
    "        return self.__portfolio_value\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self.__loss\n",
    "\n",
    "    @property\n",
    "    def layers_dict(self):\n",
    "        return self.__net.layers_dict\n",
    "\n",
    "    def recycle(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.__net.session.close()\n",
    "\n",
    "        \n",
    "   \n",
    "    \n",
    "    # Define the optimizer operation (train_step)\n",
    "    def init_train(self, learning_rate, decay_steps, decay_rate, training_method):\n",
    "        learning_rate = tf.train.exponential_decay(learning_rate, self.__global_step,\n",
    "                                                   decay_steps, decay_rate, staircase=True)\n",
    "        if training_method == 'GradientDescent':\n",
    "            train_step = tf.train.GradientDescentOptimizer(learning_rate).\\\n",
    "                         minimize(self.__loss, global_step=self.__global_step)\n",
    "        elif training_method == 'Adam':\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate).\\\n",
    "                         minimize(self.__loss, global_step=self.__global_step)\n",
    "        elif training_method == 'RMSProp':\n",
    "            train_step = tf.train.RMSPropOptimizer(learning_rate).\\\n",
    "                         minimize(self.__loss, global_step=self.__global_step)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        return train_step\n",
    "\n",
    "    # Train the NN using the previously defined train_operation\n",
    "    def train(self, x, y, last_w, setw):\n",
    "        tflearn.is_training(True, self.__net.session)\n",
    "        self.evaluate_tensors(x, y, last_w, setw, [self.__train_operation])\n",
    "\n",
    "        \n",
    "    # Before feeding the tensor into the NN, checks that there is no nan values\n",
    "    def evaluate_tensors(self, x, y, last_w, setw, tensors):\n",
    "        \"\"\"\n",
    "        :param x: input tensor\n",
    "        :param y: relative price rank 2 tensor\n",
    "        :param last_w: previous weight to consider transaction costs\n",
    "        :param setw: a function, pass the output w to it to fill the PVM\n",
    "        :param tensors: other tensors so as to update its values when running the training (see TrainTrader.ipynb)\n",
    "        :return: The result of runing the NN (weight vector)\n",
    "        \"\"\"\n",
    "        tensors = list(tensors)\n",
    "        tensors.append(self.__net.output)\n",
    "        assert not np.any(np.isnan(x))\n",
    "        assert not np.any(np.isnan(y))\n",
    "        assert not np.any(np.isnan(last_w)), \"the last_w is {}\".format(last_w)\n",
    "        # The session which is ran by this function is the session of the NN class, so it computes the operations defined\n",
    "        # in that class which are related to the tensors given in the feed_dict. In this case, it returns the weight vector\n",
    "        # computed using the parameter tensors which has to be the train_step.\n",
    "        results = self.__net.session.run(tensors, feed_dict={\n",
    "                                         self.__net.input_tensor: x,          # Enters the net\n",
    "                                          self.__Y: y,\n",
    "                                          self.__net.previous_w: last_w,      # Input of EIIE_Output_WithW together with x\n",
    "                                          self.__net.input_num: x.shape[0]})  # Num batches in the dataset\n",
    "        \n",
    "        # TODO: por que el ultimo elemento. No te devuelve como ultimo elemento el num de batches?\n",
    "        # For all the batches, all the weights in the weight vector (last element in the reward list) but the first one\n",
    "        # (weight associated with cash) is stored in the PVM once it has been computed by the NN.\n",
    "        setw(results[-1][:, 1:])   \n",
    "        return results[:-1]\n",
    "\n",
    "    \n",
    "    # Save the variables path including file name\n",
    "    def save_model(self, path):\n",
    "        self.__saver.save(self.__net.session, path)\n",
    "\n",
    "        \n",
    "   \n",
    "    \n",
    "    # The history is a 3d matrix (tensor of rank 4). It returns an asset vector in which the agent should invert \n",
    "    def decide_by_history(self, history, last_w):\n",
    "        assert isinstance(history, np.ndarray),\\\n",
    "            \"the history should be a numpy array, not %s\" % type(history)\n",
    "        assert not np.any(np.isnan(last_w))\n",
    "        assert not np.any(np.isnan(history))\n",
    "        tflearn.is_training(False, self.session)\n",
    "        history = history[np.newaxis, :, :, :]\n",
    "        # The session which is ran by this function is the session of this class, so computes the operations defined\n",
    "        # in this class which are related to the tensors given in the feed_dict\n",
    "        return np.squeeze(self.session.run(self.__net.output, feed_dict={self.__net.input_tensor: history,\n",
    "                                                                         self.__net.previous_w: last_w[np.newaxis, 1:],\n",
    "                                                                         self.__net.input_num: 1}))\n",
    "        \n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
