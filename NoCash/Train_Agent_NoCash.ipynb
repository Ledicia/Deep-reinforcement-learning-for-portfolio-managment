{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents the main part of the project. It is decomposed in the following parts:\n",
    "- Parameters setting \n",
    "- Creation of the trading environment \n",
    "- Set-up of the trading agent (simple_agent)\n",
    "- Set-up of the portfolio vector memory (PVM)\n",
    "- Agent training \n",
    "- Agent Evaluation\n",
    "- Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Note:</u> This notebook has been cleaned up and run on a local machine. The appearing results are only for illustration and not representative of the project results in the presentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BRBAcvyDeOBF"
   },
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T00:38:45.305452Z",
     "start_time": "2018-07-08T00:38:41.954075Z"
    },
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1cA1tOpgeOBG",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from PVM.ipynb\n"
     ]
    }
   ],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import ipynb.fs.full\n",
    "import import_ipynb\n",
    "from PVM import PVM\n",
    "from Agents.DPG_NoCash import DPG\n",
    "from MarketEnvironment import MarketEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T00:38:45.305452Z",
     "start_time": "2018-07-08T00:38:41.954075Z"
    },
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1cA1tOpgeOBG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ffn\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "# Import backend to build neural networs and tensors\n",
    "import tensorflow as tf\n",
    "\n",
    "# Imports for plotting\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xU0NVhPpd4y6"
   },
   "source": [
    "# 2. Load Data and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T00:38:45.721074Z",
     "start_time": "2018-07-08T00:38:45.307419Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load market data tensor \n",
    "periods = '2012-01-01_2020-01-01'\n",
    "path_data = './MarketData/np_data/input_' + periods + '_closeNorm.npy'\n",
    "# path_data = './MarketData/np_data/input_2014-01-01_2020-06-23.npy'\n",
    "# path_data = './MarketData/np_data/input_2008-01-01_2010-01-01.npy'\n",
    "data = np.load(path_data)       # Load data\n",
    "trading_period = data.shape[2]  # All the periods for which feature prices are recorded\n",
    "num_features = data.shape[0]-1   # Number of feature prices\n",
    "num_assets = data.shape[1]      # Number of market assets\n",
    "\n",
    "m = num_assets\n",
    "asset_list = ['SAN.MC', 'TEF.MC', 'GRF.MC', 'BIO.MC', 'FAE.MC', 'AIR.MC', 'IDR.MC', 'ITX.MC', 'SLR.MC']\n",
    "# asset_list = [i for i in range(m)]\n",
    "\n",
    "training_validating_testing = {'train_ratio': 0.6, 'validation_ratio': 0.2}\n",
    "training_parameters = {'num_epochs': 10, 'num_batches': 30, 'batch_size': 20}\n",
    "network_parameters = {'regularization': 1e-8, 'learning': 9e-4, 'window_size': 50}\n",
    "financial_information = {'trading_cost': 0.25/100, 'interest_rate': 0.02/250}\n",
    "investing_information = {'initial_portfolio_value': 10000, 'initial_weight_vector': np.array(np.array([1]+[0]*m))}\n",
    "\n",
    "# Trading steps/periods (days) for training, validations and testing sets:\n",
    "training_set_periods = int(training_validating_testing['train_ratio']*trading_period)\n",
    "validating_set_periods = int(training_validating_testing['validation_ratio']*trading_period)\n",
    "testing_set_periods = trading_period-training_set_periods - validating_set_periods\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = training_parameters['num_epochs']\n",
    "num_batches = training_parameters['num_batches']\n",
    "batch_size = training_parameters['batch_size']\n",
    "\n",
    "# Network optimization parameters:\n",
    "regularization = network_parameters['regularization']  # The L2 regularization coefficient applied to network training\n",
    "learning = network_parameters['learning']              # Parameter alpha (i.e. the step size) of the Adam optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning)\n",
    "n = network_parameters['window_size']\n",
    "\n",
    "# Financial parameters\n",
    "trading_cost = financial_information['trading_cost']\n",
    "interest_rate = financial_information['interest_rate']\n",
    "\n",
    "# Investing_information\n",
    "w_init = investing_information['initial_weight_vector']           # Before starting to trade, all the portfolio is composed by cash \n",
    "pf_value_init = investing_information['initial_portfolio_value']  # Amount of cash the agent is going to invest\n",
    "\n",
    "# PVM Parameters\n",
    "sample_bias = 5e-5  # Beta in the geometric distribution for mini-batch training sample batches\n",
    "\n",
    "import datetime\n",
    "Dates = pd.read_csv('MarketData/Madrid_SE_' + periods + '/' + 'AIR.MC.csv',).Date\n",
    "Dates = Dates[:-1]\n",
    "train_dates = list(Dates[n : training_set_periods])\n",
    "val_dates = list(Dates[training_set_periods + n : training_set_periods + validating_set_periods])\n",
    "test_dates = list(Dates[training_set_periods + validating_set_periods + n : ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train Agent\n",
    "\n",
    "First the action is going to be computed without training the net, and its performance is going to be evaluated. Then, after each batch the NN is going to update its parameters and evaluate its performance again.\n",
    "\n",
    "## 3.1 Evaluate performance of the agent\n",
    "\n",
    "For each period of the validation set, the agent is going to read $X_t = [:, :, t-n:t]$ which is the tensor containing the n previous relative prices for all the assets and the features, and the previous action taken by the agent in the last period, and compute the new action using the NN trained parameters (they already have been trained when this function is triggered). Since the NN needs the input tensors to be rank 4 tensors because training is doing using the mini batch technique (explained on the training function), the rank 3 $X_t$ tensor is rechaped into a rank 4 X_t tensor such that $X_t = [:, :, t-n:t] = [1, :, :, t-n:t]$ and the $W_{previous}$ vector (rank 1 tensor) is going to be reshaped into a rank 2 one $W_{previous} = (asset_0, asset_1, \\dots , asset_m) = [Assets] = [1, Assets] $. The reshape operation does not suppose any changes in the values because only one dimension to the batch dimension is added, so here they are exactly the same tensors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "def get_max_draw_down(xs):\n",
    "    xs = np.array(xs)\n",
    "    i = np.argmax(np.maximum.accumulate(xs) - xs) # end of the period\n",
    "    j = np.argmax(xs[:i])                         # start of period\n",
    "    \n",
    "    return xs[j] - xs[i]\n",
    "    \n",
    "    \n",
    "def eval_performance(epoch, agent, LogReturn = True):\n",
    "    \n",
    "    # Create empty lists so as to evaluate the performance of the actor \n",
    "    list_weight_end_val = list()\n",
    "    list_pf_end_training = list()\n",
    "    list_pf_min_training = list()\n",
    "    list_pf_max_training = list()\n",
    "    list_pf_mean_training = list()\n",
    "    list_pf_dd_training = list()\n",
    "    list_sharpe_ratios = list()\n",
    "    \n",
    "    # Create the trading environment and initializate it with initial weight and portfolio value\n",
    "    # n + t is where it starts reading the data\n",
    "    env = MarketEnvironment(path = path_data, window_length = n,\n",
    "                   initial_portfolio_value = pf_value_init, trading_cost = trading_cost,\n",
    "                   interest_rate = interest_rate, train_size = training_validating_testing['train_ratio'], LogReturn = LogReturn)\n",
    "    # The first period that the net is going to compute is training_set_periods + n \n",
    "    # cause it needs the n previous periods to compute tha action\n",
    "    state, done = env.reset(w_init[1:], pf_value_init, t = training_set_periods)\n",
    "\n",
    "    # First element of the portfolio value and the weight vector\n",
    "    p_list = [pf_value_init]\n",
    "    w_list = [w_init[1:]]\n",
    "\n",
    "    # Reads from training_set_periods (first validating set index) + n \n",
    "    # until the last validating set index minus n because the last index of the market environment is\n",
    "    # MarketEnvironment.index = training_set_periods + validating_set_periods + n - n\n",
    "    # And X_last = data[:,:,index-n:index]\n",
    "    for k in range(training_set_periods + 1, training_set_periods + validating_set_periods - n):\n",
    "        # Reshape the tensors adding one dimension for the batches to feed the into the NN\n",
    "        X_t = state[0].reshape([-1] + list(state[0].shape))\n",
    "        X_t = X_t[:, :-1, :, :]\n",
    "        W_previous = state[1].reshape([-1]+ list(state[1].shape))\n",
    "        \n",
    "        # Compute the action the agent takes (once the parameters of the net have been trained) \n",
    "        action = agent.compute_W(X_t, W_previous)\n",
    "       \n",
    "        # Forward step: compute new environment state\n",
    "        state, reward, done = env.step(action)\n",
    "\n",
    "        X_next = state[0]                  # X[:, :, t+1-n:t+1]\n",
    "        W_t = state[1]                     # Action at the end of the trading period (contains price fluctuations)\n",
    "        pf_value_t  = state[2]             # Portfolio value at the end of the trading period\n",
    "        dailyReturn_t = X_next[-1, :, -1]  # X[opening/opening, :, t+1] = opening(t+1)/opening(t)\n",
    "        \n",
    "        #print('current portfolio value', round(pf_value_previous,0))\n",
    "        #print('weights', W_previous)\n",
    "        p_list.append(pf_value_t)  # List of the portfolio values after each step\n",
    "        w_list.append(W_t)         # List of the actions taken by the agent after each step\n",
    "        list_sharpe_ratios.append(agent.get_sharpe_ratio)\n",
    "        \n",
    "    # Record just the last element. Lists are created to find out the max, min and mean values of the portfolio\n",
    "    list_weight_end_val.append(w_list[-1])\n",
    "    list_pf_end_training.append(p_list[-1])\n",
    "    list_pf_min_training.append(np.min(p_list))\n",
    "    list_pf_max_training.append(np.max(p_list))\n",
    "    list_pf_mean_training.append(np.mean(p_list))\n",
    "    \n",
    "    list_pf_dd_training.append(get_max_draw_down(p_list))\n",
    "    dates = [datetime.datetime.strptime(d, \"%Y-%m-%d\").date() for d in val_dates]\n",
    "    print('End of test PF value:',round(p_list[-1]))\n",
    "    print('Min of test PF value:',round(np.min(p_list)))\n",
    "    print('Max of test PF value:',round(np.max(p_list)))\n",
    "    print('Mean of test PF value:',round(np.mean(p_list)))\n",
    "    print('Max Draw Down of test PF value:',round(get_max_draw_down(p_list)))\n",
    "    print('End of test weights:',w_list[-1])\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title('Portfolio evolution (validation set) episode {}'.format(epoch))\n",
    "    plt.xlabel('Periods')\n",
    "    plt.ylabel('Portfolio Value')\n",
    "    ax.plot(dates, p_list, label = 'Agent Portfolio Value')\n",
    "    fig.autofmt_xdate()                              # Rotate and align the tick labels so they look better\n",
    "    ax.fmt_xdata = mdates.DateFormatter('%Y-%m-%d')  # Format dates to string for the x axis locations \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title('Portfolio weights (end of validation set) episode {}'.format(epoch))\n",
    "    plt.ylabel('Weights')\n",
    "    plt.bar(np.arange(m), list_weight_end_val[-1])\n",
    "    plt.xticks(np.arange(m), asset_list, rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    names = asset_list\n",
    "    w_list = np.array(w_list)\n",
    "    for j in range(m):\n",
    "        plt.plot(w_list[:,j], label = 'Weight Stock {}'.format(names[j]))\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train the NN using batch training\n",
    "\n",
    "1. A batch starting with period $tb$ $t_0 âˆ’ n_b$ is picked with a geometrically distributed probability (PVM class).\n",
    "2.  It is important that prices inside a batch are in time-order: the slices of the X_t tensor are selected such that X_t[:,:,index:index+n] where the third dimension is the time dimension. However, the index for training is choosen by a geometrical random function.\n",
    "3. The for loop which runs through bs index appends into lists the results from each sample in the batch. The lists are converted into arrays that are fed into the NN (and treated as rank 4 tensors):\n",
    "- list_X_t = $(X[:,:,index:index+n], X[:,:,index+1: index+1 + n],\\dots , X[:,:,tb+batch_size: tb+batch\\_size + n])$ where $index = n + tb$\n",
    "- list_W_t = $(W\\_index, W\\_{index+1}, \\dots , W\\_{index+batch\\_size})$  $\\Rightarrow$ Each of this W are calculated at the end of each period (considerinf the evolution of the price during the session)\n",
    "4. train: calls train function defined in the agents class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random action function\n",
    "def get_random_action(m):\n",
    "    random_vec = np.random.rand(m)\n",
    "    return random_vec/np.sum(random_vec)\n",
    "\n",
    "\n",
    "def train(agent, env, num_epochs, eval_performance):\n",
    "\n",
    "    # Keep record of the values of the portfolio at the end of each batch\n",
    "    list_final_pf_batch_values = list()\n",
    "    \n",
    "    # Number of epochs (times the whole dataset is computed)\n",
    "    for e in range(num_epochs):\n",
    "        # Evaluate the performance of the agent before it has been trained \n",
    "        # The action is computed just feeding the X_t and W_previous tensors of the validation set\n",
    "        print('Start Episode', e)\n",
    "        if e==0:\n",
    "            eval_performance('Before Training', agent)\n",
    "        print('Episode:', e)\n",
    "\n",
    "        # Init the PVM with the training parameters (initialize it after each epoch)\n",
    "        memory = PVM(m, sample_bias, total_steps = training_set_periods, batch_size = batch_size, w_init = w_init[1:])\n",
    "\n",
    "        # Number of batches: For each batch (nb), the net sees a determined number of samples from training set\n",
    "        for nb in range(num_batches):\n",
    "            # Starting point of the batch: get a ramdom index to initialize the batch\n",
    "            t_start = memory.get_random_index()\n",
    "\n",
    "            # Initialize the environment with the weight from PVM at the starting point and the initial portfolio value\n",
    "            # Starts reading data from t_start + n (n is defined in the env which is passed to the function)\n",
    "            state, done = env.reset(memory.get_W(t_start), pf_value_init, t=t_start)\n",
    "\n",
    "            # Initialize lists to store the values at each step/sample of the batch\n",
    "            list_X_t, list_W_previous, list_pf_value_previous, list_dailyReturn_t = [], [], [], []\n",
    "\n",
    "            # Read the samples in the batch to create the data structures needed to train the NN\n",
    "            for bs in range(batch_size):\n",
    "                \n",
    "                # Keep in mind that what it is called here t, in the MarketEnvironment is t_start+n\n",
    "                # X_t [:,:,t-n:t] = [:,:,t_start:t_start+n]\n",
    "                # Reshapes X_t [features, assets, periods] into [1, features, assets, periods] \n",
    "                # because the nn needs to receive a rank 4 tensor to compute the actionfor the next period\n",
    "                X_t = state[0].reshape([-1] + list(state[0].shape))\n",
    "                X_t = X_t[:, :-1,:, :]\n",
    "                W_previous = state[1].reshape([-1] + list(state[1].shape))\n",
    "                pf_value_previous = state[2]\n",
    "\n",
    "                # Compute actions \n",
    "                # Take random action sometimes to improve exploration\n",
    "                if np.random.rand() < 0.6:\n",
    "                    action = agent.compute_W(X_t, W_previous)\n",
    "                else:\n",
    "                    action = get_random_action(m)\n",
    "                \n",
    "                # Given the state and the action, call the environment to go one time step later (compute next state and reward)\n",
    "                # The results from action w_t (X_next, W_t at the end of the period and pf_value_t at the end of the period) are\n",
    "                # stored in the following variables so, for the next sample of the for loop, action w_{t+1} is computed with data\n",
    "                # X_next = X[1,:,:, t + 1: t + 1 + window_length] and w_t (at the end of the period)\n",
    "                state, reward, done = env.step(action)\n",
    "                \n",
    "                # Get the action once it has evolved, and the portfolio value\n",
    "                # The action is needed to pass it into the NN so as it can learn\n",
    "                # The portfolio value is just for drawing the evolution\n",
    "                W_t = state[1]         # Portfolio weight vector at the end of period t (considering evolution of prices)\n",
    "                pf_value_t = state[2]  # Portfolio value at the end of period t(considering evolution of prices)\n",
    "\n",
    "                # Get the daily return: X_next = X[:,:,t+1-n:t+1] where t = t_start + n\n",
    "                # DailyReturn_t = X_next[-1,:,-1] = [open(t+1)/open(t)] price fluctuation during session t (~ close(t)/open(t))\n",
    "                # DailyReturn_t-1 = X_t[-1,:,-1] = [open(t)/open(t-1)] \n",
    "                X_next = state[0]\n",
    "                dailyReturn_t = X_next[-1, :, -1]\n",
    "                \n",
    "                # Each X_t tensor is a rank 3 tensor of [features, assets, window length]\n",
    "                # Store each sample elements so as to use them to train the net as a batch\n",
    "                list_X_t.append(X_t.reshape(state[0].shape[0]-1, state[0].shape[1], state[0].shape[2]))\n",
    "                list_W_previous.append(W_previous.reshape(state[1].shape))\n",
    "                list_pf_value_previous.append([pf_value_previous])\n",
    "                list_dailyReturn_t.append(dailyReturn_t)\n",
    "\n",
    "                # End of the batch: keep record of the portfolio values at the end of each batch\n",
    "                if bs==batch_size-1:\n",
    "                    list_final_pf_batch_values.append(pf_value_t)\n",
    "\n",
    "                # Update this action (once it has evolved) into the PVM so as to use it to evaluate next action\n",
    "                memory.update(t_start + bs, W_t)\n",
    "            \n",
    "            # Build the rank 4 tensors: batches composed by bs samples \n",
    "            list_X_t = np.array(list_X_t)\n",
    "            list_W_previous = np.array(list_W_previous)\n",
    "            list_pf_value_previous = np.array(list_pf_value_previous)\n",
    "            list_dailyReturn_t = np.array(list_dailyReturn_t)\n",
    "           \n",
    "\n",
    "            #for each batch, train the network to maximize the reward\n",
    "            agent.train(list_X_t, list_W_previous,\n",
    "                        list_pf_value_previous, list_dailyReturn_t)\n",
    "            agent.save_model()\n",
    "            print(nb)\n",
    "            first_layer = agent.get_first_layer(X_t, W_previous); print('1', first_layer.shape)\n",
    "#             if np.any(first_layer)==0:\n",
    "#                 print('WARNING, first layer')\n",
    "#             second_layer = agent.get_second_layer(X_t, W_previous);# print(second_layer)\n",
    "#             if np.any(second_layer)==0:\n",
    "#                 print('WARNING, second_layer')\n",
    "            growth_potential = agent.get_growth_potential(X_t, W_previous); print(growth_potential)\n",
    "            if np.any(growth_potential)==0:\n",
    "                print('WARNING, Network is not computing the potential of growth of the assest!')\n",
    "            print('Sharpe Ratio over batch: ', agent.get_sharpe_ratio(list_X_t, list_W_previous,\n",
    "                        list_pf_value_previous, list_dailyReturn_t))\n",
    "            print('Loss function over batch: ', agent.get_loss_function(list_X_t, list_W_previous,\n",
    "                        list_pf_value_previous, list_dailyReturn_t))\n",
    "            \n",
    "        eval_performance(e, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Create environment and instanciate the agent\n",
    "\n",
    "The environment is in charge of computing the tensors $X_t$, $W_{previous}$ and $pf_{previous}$ that are going to be fed into the NN. Once the batch tensors are created they are fed into the NN for training. Therefore, the net is not going to update the parameters of the layers until a number equal of batch_size of tensors have passed through its layers. Once the net has been trained over a determined number of epochs (number of times the net seed the whole dataset), the training is complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ledic\\Anaconda3\\lib\\site-packages\\tflearn\\layers\\conv.py:73: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ledic\\Anaconda3\\lib\\site-packages\\tflearn\\initializations.py:119: calling UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\ledic\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py:507: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From <ipython-input-6-378e12b014b7>:104: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ledic\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-378e12b014b7>:106: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "Loading Model\n",
      "Saved to:ModelParams_LR_closeNorm_/\n",
      "model_checkpoint_path: \"ModelParams_LR_closeNorm_/Model2_n50_s9_9e-4_bs20_reg2_retrained_.ckpt\"\n",
      "all_model_checkpoint_paths: \"ModelParams_LR_closeNorm_/Model2_n50_s9_9e-4_bs20_reg2_retrained_.ckpt\"\n",
      " ModelParams_LR_closeNorm_/Model2_n50_s9_9e-4_bs20_reg2_retrained_.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ModelParams_LR_closeNorm_/Model2_n50_s9_9e-4_bs20_reg2_retrained_.ckpt\n",
      "Successfully loaded: ModelParams_LR_closeNorm_/Model2_n50_s9_9e-4_bs20_reg2_retrained_.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Initialize session and run it so as to train the agent\n",
    "tf.reset_default_graph()\n",
    "device = 'cpu'\n",
    "path_to_save = 'ModelParams_LR_closeNorm_/'\n",
    "model_name = 'Model2_n50_s9_9e-4_bs20_reg2_retrained_.ckpt'; \n",
    "env_log = MarketEnvironment(path = path_data, window_length = n,\n",
    "               initial_portfolio_value = pf_value_init, trading_cost = trading_cost,\n",
    "               interest_rate = interest_rate, train_size = training_validating_testing['train_ratio'], LogReturn = True)\n",
    "\n",
    "load_weights = True\n",
    "if device == \"cpu\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        log_agent = DPG(num_features, m, n, device, optimizer, trading_cost, interest_rate, \n",
    "                       path_to_save, model_name, LogReturn = True, load_weights = load_weights)\n",
    "else:\n",
    "    log_agent = DPG(num_features, m, n, device, optimizer, trading_cost, interest_rate, \n",
    "                   path_to_save, model_name, LogReturn = True, load_weights = load_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Episode 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8fb5d753f6ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_log\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_performance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-3078f7df324e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(agent, env, num_epochs, eval_performance)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Start Episode'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0meval_performance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Before Training'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Episode:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-000635f414db>\u001b[0m in \u001b[0;36meval_performance\u001b[1;34m(epoch, agent, LogReturn)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# Compute the action the agent takes (once the parameters of the net have been trained)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_W\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_previous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# Forward step: compute new environment state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-378e12b014b7>\u001b[0m in \u001b[0;36mcompute_W\u001b[1;34m(self, X_t_, W_previous_)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;31m# Compute the agent's action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_W\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_t_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_previous_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_t\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_t_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_previous\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mW_previous_\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;31m# Train the NN maximizing the reward: the input is a batch of the differents values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(log_agent, env_log, 20, eval_performance)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "DPM_v2.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "219px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "696px",
    "left": "611.992px",
    "right": "20px",
    "top": "113.984px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
