{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "import tflearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NN Class\n",
    "Allows building the network used in the experiment from the parameters specify instead of defining all the variables:\n",
    "* feature_number: number of features imputing the net\n",
    "* rows: for the portfolio management problem it represents the number of non cash assets\n",
    "* columns: for the portfolio management problem it represents the number of previous data\n",
    "* layers: layers ofthe NN\n",
    "* device: cpu or gpu\n",
    "\n",
    "## Mini-Batch training:\n",
    "* Training set (TS): Number of samples (previous periods), that are fed into the NN.\n",
    "* Batch training: The network evaluates a number of samples nb $\\subset$ TS before updating its parameter vector $\\vec{w}$ which is nothing but the action taken by the agent. After Nb batch trainings of size nb (the lasy one could be smaller), the network has seen the hole training set TS.\n",
    "* Number of epochs: The training set TS should be seen for the network a number of epochs so as to update parameters and reduce loss, improving accuracy.\n",
    "\n",
    "Since the mini batch training is implemented, the tensors fed into the NN are going to have an extra dimension, which is the dimension 0 of the tensors, and represents the number of samples (periods) in that batch. So actually, the tensor still is going to have 3 dimensions of information $[features, assets, periods]$, but is going to be splitted into Nb batches of nb samples. Let's imagine that the number of periods/ samples that are going to be fed into the net goes from 1 to 100, and that the dataset is splitted into 4 batches, then:\n",
    "\n",
    "Epoch 1 inputs 4 tensors of 3 dimensions:\n",
    "- Batch 1: $P_{1b} = [p1, ... , p25]$ $\\Rightarrow$ Computes $\\vec{w}$ (vector or rank 1 tensor)\n",
    "- Batch 2: $P_{2b} = [p26, ... , p50]$ $\\Rightarrow$ Updates $\\vec{w}$\n",
    "- Batch 3: $P_{3b} = [p51, ... , p75]$ $\\Rightarrow$ Updates $\\vec{w}$\n",
    "- Batch 4: $P_{4b} = [p76, ... , p100]$ $\\Rightarrow$ Updates $\\vec{w}$\n",
    "\n",
    "Where $p_i$ is a tensor $[assets, features]$ and $P_nb$ a tensor $[assets, nb periods, features]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetWork:\n",
    "    \n",
    "    def __init__(self, feature_number, rows, columns, layers, device):\n",
    "        \n",
    "        ## Configure the session to run the NN\n",
    "        tf_config = tf.ConfigProto()\n",
    "        self.session = tf.Session(config=tf_config)\n",
    "        if device == \"cpu\":\n",
    "            tf_config.gpu_options.per_process_gpu_memory_fraction = 0\n",
    "        else:\n",
    "            tf_config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "            \n",
    "        ## Placeholders of the NN: Tensors that will be feed into the NN\n",
    "        self.input_num = tf.placeholder(tf.int32, shape=[])  # Defines the number of samples in a batch\n",
    "        self.input_tensor = tf.placeholder(tf.float32, shape=[None, feature_number, rows, columns])\n",
    "        self.previous_w = tf.placeholder(tf.float32, shape=[None, rows])  \n",
    "        self._rows = rows        \n",
    "        self._columns = columns\n",
    "\n",
    "        self.layers_dict = {}  # Keep track of the layers used\n",
    "        self.layer_count = 0   # Count the number of layers (this parameter is updated after every layer)\n",
    "\n",
    "        self.output = self._build_network(layers)  # Returns the result of running the net (computes w)\n",
    "\n",
    "    ## Function that specifies how the NN is built\n",
    "    # Here it is not specified because this class is only for inheriting properties\n",
    "    # Below a specific NN for the portfolio management problem is oing to be defined\n",
    "    def _build_network(self, layers):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T21:36:22.007862Z",
     "start_time": "2018-05-02T21:36:19.154744Z"
    }
   },
   "outputs": [],
   "source": [
    "#for navigation in the folders\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from time import strptime\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#for plot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import PIL\n",
    "import pickle\n",
    "from time import strftime\n",
    "\n",
    "import seaborn as sns\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CNN - Read Network architechture\n",
    "This class inherits directly from the NN class, and therefore has its properties. \n",
    "It defines how to build the NN depending on the layer dict from the configuration json file, and computes the weight vector $\\vec{w}$ by calling the function _build_network.\n",
    "\n",
    "## 2.1 Architechture of the net:\n",
    "The architechture is defined in a configuration json file, which has a subkeycalled layers containing the network architechture and the information needed to compute each layer.\n",
    "\n",
    "The input tensor is fed into the network, reshaped and normalized by the closing price of the actual period $t$ ($[bathes, assets, window\\_size, features]$). This tensor inputs the first layer of the NN.\n",
    "\n",
    "The possible layers implemented are the following:\n",
    " \n",
    "1. Pass the input through a convolutional layer\n",
    "2. EIIE_Dense: Computes each asset separately so as to evaluate its potential of growth without being subject to the other assets. Therefore, each computation can be thought as a fully connected layer computation with as many parameters as previous periods are in a batch.\n",
    " - Each IIE computes a row (an asset) sharing parameters among the assets (advantage of a CNN) -> \"dense layer\"\n",
    " - To do so, the filter size is $[1, numcols]$ meaning that each convolution operation takes 1 row, and numcols (the number of previous information about a partcular asset in the batch).\n",
    "           \n",
    "3. EIIE_Output_WithW: Input previous weights, compute voting score, input cash bias, and pass this through a softmax layer. The output is a portfolio weight vector with shape $[Batches, 1+m]$ (one vector for each batch).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(NeuralNetWork):\n",
    "    \n",
    "    def __init__(self, feature_number, rows, columns, layers, device):\n",
    "        NeuralNetWork.__init__(self, feature_number, rows, columns, layers, device)  # A CNN inherits from NN\n",
    "\n",
    "        \n",
    "    # Add layers to the dict used to construct the net and keep track of the layers used in the NN specified in the config file\n",
    "    def add_layer_to_dict(self, layer_type, tensor, weights=True):\n",
    "        self.layers_dict[layer_type + '_' + str(self.layer_count) + '_activation'] = tensor\n",
    "        self.layer_count += 1\n",
    "\n",
    "        \n",
    "    # Generate the NN (forward computation)\n",
    "    def _build_network(self, layers):\n",
    "        # perm = [0, 2, 3, 1] which means dim0=dim0, dim1=dim2, dim2=dim3, dim3=dim1\n",
    "        # reshape the input tensor from [batch, features, assets, window] to [batch, assets, window, features] \n",
    "        network = tf.transpose(self.input_tensor, [0, 2, 3, 1])\n",
    "\n",
    "        # The network tensor is a rank4 tensor [batch, assets, window, features] \n",
    "        # Normalize the tensor values by dividing them by the closing price (feature 0) of the current epriod t \n",
    "        # (last element o the window_size) for all the batches and assets. \n",
    "        network = network / network[:, :, -1, 0, None, None]    # Normalize the input tensor\n",
    "                \n",
    "        # Build the net from a json file (config):\n",
    "        # The following layers are the possible layers for building the trader agent\n",
    "        # Depending on a json file with the combinations of the layers different NN will be constructed\n",
    "        for layer_number, layer in enumerate(layers):\n",
    "            # Inputting data to a layer:\n",
    "            # Each layer inputs a tensor (network) and outputs another tensor with the same name (network)\n",
    "            # Names of the input and output are the same so as to input always the computed output from the previous layer\n",
    "            \n",
    "            # Dense layer: Normal DeepNeuralNetwork layer\n",
    "            if layer[\"type\"] == \"DenseLayer\":\n",
    "                network = tflearn.layers.core.fully_connected(network,\n",
    "                                                              int(layer[\"neuron_number\"]),\n",
    "                                                              layer[\"activation_function\"],\n",
    "                                                              regularizer=layer[\"regularizer\"],\n",
    "                                                              weight_decay=layer[\"weight_decay\"] )\n",
    "                self.add_layer_to_dict(layer[\"type\"], network)\n",
    "              \n",
    "            # Regularization: Only for training\n",
    "            elif layer[\"type\"] == \"DropOut\":\n",
    "                network = tflearn.layers.core.dropout(network, layer[\"keep_probability\"])\n",
    "              \n",
    "            # EIIE_Dense: Ensemble identical independent evaluators, which behaves as a dense (fully connected) layer \n",
    "            elif layer[\"type\"] == \"EIIE_Dense\":\n",
    "                width = network.get_shape()[2]  # dimension of the third element of the shape (related to the historic periods)\n",
    "                network = tflearn.layers.conv_2d(network, int(layer[\"filter_number\"]),\n",
    "                                                 [1, width],  # filter size\n",
    "                                                 [1, 1],      # stride\n",
    "                                                 \"valid\",\n",
    "                                                 layer[\"activation_function\"],\n",
    "                                                 regularizer=layer[\"regularizer\"],\n",
    "                                                 weight_decay=layer[\"weight_decay\"])\n",
    "                self.add_layer_to_dict(layer[\"type\"], network)\n",
    "                \n",
    "            # ConvLayer: Normal convolutional layer\n",
    "            elif layer[\"type\"] == \"ConvLayer\":\n",
    "                network = tflearn.layers.conv_2d(network, int(layer[\"filter_number\"]),\n",
    "                                                 allint(layer[\"filter_shape\"]),\n",
    "                                                 allint(layer[\"strides\"]),\n",
    "                                                 layer[\"padding\"],\n",
    "                                                 layer[\"activation_function\"],\n",
    "                                                 regularizer=layer[\"regularizer\"],\n",
    "                                                 weight_decay=layer[\"weight_decay\"])\n",
    "                self.add_layer_to_dict(layer[\"type\"], network)\n",
    "              \n",
    "            # Reduce dimensionality allowing for assumptions to be made about features contained in the sub-regions binned\n",
    "            elif layer[\"type\"] == \"MaxPooling\":\n",
    "                network = tflearn.layers.conv.max_pool_2d(network, layer[\"strides\"])\n",
    "                \n",
    "            # Reduce dimensionality allowing for assumptions to be made about features contained in the sub-regions binned\n",
    "            elif layer[\"type\"] == \"AveragePooling\":\n",
    "                network = tflearn.layers.conv.avg_pool_2d(network, layer[\"strides\"])\n",
    "             \n",
    "            # Normalize the inputs\n",
    "            elif layer[\"type\"] == \"LocalResponseNormalization\":\n",
    "                # The 4-D input tensor is treated as a 3-D array of 1-D vectors, and each vector is normalized independently\n",
    "                # Within a given vector, each component is divided by the weighted, squared sum of inputs within depth_radius\n",
    "                network = tflearn.layers.normalization.local_response_normalization(network)\n",
    "            \n",
    "            # EIIE_Output: generates the potential of growth of each asset individually without considering previous weights\n",
    "            # Then, adds the cash bias and outputs the new weight vector \n",
    "            elif layer[\"type\"] == \"EIIE_Output\":\n",
    "                width = network.get_shape()[2]  # Window size\n",
    "                # Calculate the potential of growth of each asset separately\n",
    "                # Since the filter sizes are [1, width], it computes each asset individually \n",
    "                network = tflearn.layers.conv_2d(network, 1, [1, width], padding=\"valid\",\n",
    "                                                 regularizer=layer[\"regularizer\"],\n",
    "                                                 weight_decay=layer[\"weight_decay\"])\n",
    "                self.add_layer_to_dict(layer[\"type\"], network)\n",
    "                \n",
    "                ## Add the cash (btc_bias) to the asset dimension of the network tensor\n",
    "                network = network[:, :, 0, 0]                   # Gives a tensor with shape=[batches, assets]\n",
    "                btc_bias = tf.ones((self.input_num, 1))         # Shape [input_num,1]   \n",
    "                self.add_layer_to_dict(layer[\"type\"], network)\n",
    "                network = tf.concat([btc_bias, network], 1)     # [input_num, assets+1], adds bias (cash asset) to the tensor \n",
    "                # Apply given activation to incoming tensor (the cash asset has been added)\n",
    "                network = tflearn.layers.core.activation(network, activation=\"softmax\")\n",
    "                self.add_layer_to_dict(layer[\"type\"], network, weights=False)\n",
    "             \n",
    "            # Output_WithW: Adds weights and passes the network tensor through a fully connected layer (not using IIE)\n",
    "            elif layer[\"type\"] == \"Output_WithW\":\n",
    "                network = tflearn.flatten(network)\n",
    "                # Adding the previous weights in order to cosider transaction costs [network[0], network[1]+1]\n",
    "                network = tf.concat([network,self.previous_w], axis=1)\n",
    "                network = tflearn.fully_connected(network, self._rows+1,\n",
    "                                                  activation=\"softmax\",\n",
    "                                                  regularizer=layer[\"regularizer\"],\n",
    "                                                  weight_decay=layer[\"weight_decay\"])\n",
    "                \n",
    "            # EIIE_Output_WithW: Add weights, reshapes the network tensor into [input_num, assets, 1, previous periods*features]\n",
    "            # and adds the cash after computing the voting scores with the previous weights added\n",
    "            # Then, it outputs the new portfolio weight vector (action taken by the agent)\n",
    "            elif layer[\"type\"] == \"EIIE_Output_WithW\":\n",
    "                width = network.get_shape()[2]     # Window number\n",
    "                height = network.get_shape()[1]    # Asset number \n",
    "                features = network.get_shape()[3]  # Feature number\n",
    "                network = tf.reshape(network, [self.input_num, int(height), 1, int(width*features)])\n",
    "                w = tf.reshape(self.previous_w, [-1, int(height), 1, 1])  # [last batch, assets, 1, 1] \n",
    "                network = tf.concat([network, w], axis=3)                 # [last batch, assets, 1, metwork[3]+1]\n",
    "                network = tflearn.layers.conv_2d(network, 1, [1, 1],\n",
    "                                                 padding=\"valid\",         # No padding (size )\n",
    "                                                 regularizer=layer[\"regularizer\"],\n",
    "                                                 weight_decay=layer[\"weight_decay\"])\n",
    "                ## Add the cash (btc_bias) to the asset dimension of the network tensor\n",
    "                self.add_layer_to_dict(layer[\"type\"], network)\n",
    "                network = network[:, :, 0, 0]\n",
    "                #btc_bias = tf.zeros((self.input_num, 1))\n",
    "                btc_bias = tf.get_variable(\"btc_bias\", [1, 1], dtype=tf.float32, initializer=tf.zeros_initializer)\n",
    "                # self.add_layer_to_dict(layer[\"type\"], network, weights=False)\n",
    "                btc_bias = tf.tile(btc_bias, [self.input_num, 1])  # Builds a tensor by tiling a given tensor\n",
    "                network = tf.concat([btc_bias, network], 1)        # concatenates adding cols (the number of rows does not change)\n",
    "                self.voting = network\n",
    "                self.add_layer_to_dict('voting', network, weights=False)\n",
    "                network = tflearn.layers.core.activation(network, activation=\"softmax\")\n",
    "                self.add_layer_to_dict('softmax_layer', network, weights=False)\n",
    "\n",
    "            # Layer that can be used instead of the EIIE_Dense layer \n",
    "            elif layer[\"type\"] == \"EIIE_LSTM\" or\\\n",
    "                            layer[\"type\"] == \"EIIE_RNN\":\n",
    "                network = tf.transpose(network, [0, 2, 3, 1])\n",
    "                resultlist = []\n",
    "                reuse = False\n",
    "                for i in range(self._rows):\n",
    "                    if i > 0:\n",
    "                        reuse = True\n",
    "                    if layer[\"type\"] == \"EIIE_LSTM\":\n",
    "                        result = tflearn.layers.lstm(network[:, :, :, i],\n",
    "                                                     int(layer[\"neuron_number\"]),\n",
    "                                                     dropout=layer[\"dropouts\"],\n",
    "                                                     scope=\"lstm\"+str(layer_number),\n",
    "                                                     reuse=reuse)\n",
    "                    else:\n",
    "                        result = tflearn.layers.simple_rnn(network[:, :, :, i],\n",
    "                                                           int(layer[\"neuron_number\"]),\n",
    "                                                           dropout=layer[\"dropouts\"],\n",
    "                                                           scope=\"rnn\"+str(layer_number),\n",
    "                                                           reuse=reuse)\n",
    "                    resultlist.append(result)\n",
    "                network = tf.stack(resultlist)\n",
    "                network = tf.transpose(network, [1, 0, 2])\n",
    "                network = tf.reshape(network, [-1, self._rows, 1, int(layer[\"neuron_number\"])])\n",
    "            else:\n",
    "                raise ValueError(\"the layer {} not supported.\".format(layer[\"type\"]))\n",
    "        return network\n",
    "\n",
    "\n",
    "def allint(l):\n",
    "    return [int(i) for i in l]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
