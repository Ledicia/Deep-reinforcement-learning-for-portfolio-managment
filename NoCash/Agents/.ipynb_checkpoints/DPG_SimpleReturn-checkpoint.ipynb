{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(object):\n",
    "    \n",
    "    def __init__(self, nb_feature_map, m, n, sess, optimizer, trading_cost=trading_cost, interest_rate=interest_rate,\n",
    "                 n_filter_1=n_filter_1, n_filter_2=n_filter_2):\n",
    "\n",
    "        # parameters\n",
    "        self.trading_cost = trading_cost\n",
    "        self.interest_rate = interest_rate\n",
    "        self.n_filter_1 = n_filter_1\n",
    "        self.n_filter_2 = n_filter_2\n",
    "        self.num_features = nb_feature_map\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "\n",
    "        with tf.variable_scope(\"Inputs\"):\n",
    "\n",
    "            # Placeholder\n",
    "\n",
    "            # tensor of the prices\n",
    "            self.X_t = tf.placeholder(tf.float32, [None, self.num_features, self.m, self.n]) \n",
    "            # weights at the previous time step\n",
    "            self.W_previous = tf.placeholder(tf.float32, [None, self.m+1])\n",
    "            # portfolio value at the previous time step\n",
    "            self.pf_value_previous = tf.placeholder(tf.float32, [None, 1])\n",
    "            # vector of Open(t+1)/Open(t)\n",
    "            self.dailyReturn_t = tf.placeholder(tf.float32, [None, self.m])\n",
    "            \n",
    "            #self.pf_value_previous_eq = tf.placeholder(tf.float32, [None, 1])\n",
    "            \n",
    "            \n",
    "\n",
    "        with tf.variable_scope(\"Policy_Model\"):\n",
    "\n",
    "            # variable of the cash bias\n",
    "            bias = tf.get_variable('cash_bias', \n",
    "                                   shape=[1, 1, 1, 1], initializer=tf.constant_initializer(cash_bias_init))\n",
    "            # shape of the tensor == batchsize\n",
    "            shape_X_t = tf.shape(self.X_t)[0]\n",
    "            # trick to get a \"tensor size\" for the cash bias\n",
    "            self.cash_bias = tf.tile(bias, tf.stack([shape_X_t, 1, 1, 1]))\n",
    "            # print(self.cash_bias.shape)\n",
    "\n",
    "            with tf.variable_scope(\"Conv1\"):\n",
    "                # first layer on the X_t tensor\n",
    "                # return a tensor of depth 2\n",
    "                self.conv1 = tf.layers.conv2d(\n",
    "                    inputs=tf.transpose(self.X_t, perm=[0, 3, 2, 1]),\n",
    "                    activation=tf.nn.relu,\n",
    "                    filters=self.n_filter_1,\n",
    "                    strides=(1, 1),\n",
    "                    kernel_size=kernel1_size,\n",
    "                    padding='same')\n",
    "                \n",
    "            with tf.variable_scope(\"Conv2\"):\n",
    "                \n",
    "                #feature maps\n",
    "                self.conv2 = tf.layers.conv2d(\n",
    "                    inputs=self.conv1,\n",
    "                    activation=tf.nn.relu,\n",
    "                    filters=self.n_filter_2,  # 20 filters\n",
    "                    strides=(self.n, 1),\n",
    "                    kernel_size=(1, self.n), # To compute just one asset at a time\n",
    "                    padding='same')\n",
    "\n",
    "            with tf.variable_scope(\"Tensor3\"):\n",
    "                # w from last periods\n",
    "                # trick to have good dimensions\n",
    "                w_wo_c = self.W_previous[:, 1:]\n",
    "                w_wo_c = tf.expand_dims(w_wo_c, 1)\n",
    "                w_wo_c = tf.expand_dims(w_wo_c, -1)\n",
    "                self.tensor3 = tf.concat([self.conv2, w_wo_c], axis=3)\n",
    "\n",
    "            with tf.variable_scope(\"Conv3\"):\n",
    "                #last feature map WITHOUT cash bias\n",
    "                self.conv3 = tf.layers.conv2d(\n",
    "                    inputs=self.conv2,\n",
    "                    activation=tf.nn.relu,\n",
    "                    filters=1,\n",
    "                    strides=(self.n_filter_2 + 1, 1),\n",
    "                    kernel_size=(1, 1),\n",
    "                    padding='same')\n",
    "\n",
    "            with tf.variable_scope(\"Tensor4\"):\n",
    "                #last feature map WITH cash bias\n",
    "                self.tensor4 = tf.concat([self.cash_bias, self.conv3], axis=2)\n",
    "                # we squeeze to reduce and get the good dimension\n",
    "                self.squeezed_tensor4 = tf.squeeze(self.tensor4, [1, 3])\n",
    "\n",
    "            with tf.variable_scope(\"Policy_Output\"):\n",
    "                # softmax layer to obtain weights\n",
    "                self.action = tf.nn.softmax(self.squeezed_tensor4)\n",
    "\n",
    "            # Simple reward computing the difference between the previous portfolio value and this one\n",
    "            with tf.variable_scope(\"Reward\"):\n",
    "                \n",
    "                constant_return = tf.constant(1+self.interest_rate, shape=[1, 1])\n",
    "                cash_return = tf.tile(constant_return, tf.stack([shape_X_t, 1]))\n",
    "\n",
    "                y_t = tf.concat([cash_return, self.dailyReturn_t], axis=1)  # Relative price tensor for each feature\n",
    "\n",
    "                Vprime_t = self.action * self.pf_value_previous     # Value at the end of the period  t before reallocating\n",
    "                Vprevious = self.W_previous*self.pf_value_previous  # Value at the beggining of the period\n",
    "\n",
    "                constant = tf.constant(1.0, shape=[1])\n",
    "\n",
    "                cost = self.trading_cost * tf.norm(Vprime_t-Vprevious, ord=1, axis=1)*constant\n",
    "                cost = tf.expand_dims(cost, 1)\n",
    "\n",
    "                zero = tf.constant(np.array([0.0]*m).reshape(1, m), shape=[1, m], dtype=tf.float32)\n",
    "\n",
    "                vec_zero = tf.tile(zero, tf.stack([shape_X_t, 1]))\n",
    "                vec_cost = tf.concat([cost, vec_zero], axis=1)\n",
    "\n",
    "                Vsecond_t = Vprime_t - vec_cost\n",
    "\n",
    "                V_t = tf.multiply(Vsecond_t, y_t)\n",
    "                # Compute the portfolio value\n",
    "                self.portfolioValue = tf.norm(V_t, ord=1)\n",
    "                # Simple return, not logarithmic one\n",
    "                self.instantaneous_reward = (self.portfolioValue-self.pf_value_previous)/self.pf_value_previous\n",
    "\n",
    "                \n",
    "            with tf.variable_scope(\"Reward_Equiweighted\"):\n",
    "                constant_return = tf.constant(\n",
    "                    1+self.interest_rate, shape=[1, 1])\n",
    "                cash_return = tf.tile(\n",
    "                    constant_return, tf.stack([shape_X_t, 1]))\n",
    "                y_t = tf.concat(\n",
    "                    [cash_return, self.dailyReturn_t], axis=1)\n",
    "  \n",
    "\n",
    "                V_eq = w_eq*self.pf_value_previous\n",
    "                V_eq_second = tf.multiply(V_eq, y_t)\n",
    "        \n",
    "                self.portfolioValue_eq = tf.norm(V_eq_second, ord=1)\n",
    "            \n",
    "                self.instantaneous_reward_eq = (\n",
    "                    self.portfolioValue_eq-self.pf_value_previous)/self.pf_value_previous\n",
    "                \n",
    "            with tf.variable_scope(\"Max_weight\"):\n",
    "                self.max_weight = tf.reduce_max(self.action)\n",
    "                print(self.max_weight.shape)\n",
    "\n",
    "                \n",
    "            with tf.variable_scope(\"Reward_adjusted\"):\n",
    "                \n",
    "                self.adjested_reward = self.instantaneous_reward - self.instantaneous_reward_eq - ratio_regul*self.max_weight\n",
    "                \n",
    "        #objective function \n",
    "        #maximize reward over the batch \n",
    "        # min(-r) = max(r)\n",
    "        self.train_op = optimizer.minimize(-self.adjested_reward)\n",
    "        \n",
    "        # some bookkeeping\n",
    "        self.optimizer = optimizer\n",
    "        self.sess = sess\n",
    "\n",
    "        \n",
    "    def compute_W(self, X_t_, W_previous_):\n",
    "        \"\"\"\n",
    "        This function returns the action the agent takes \n",
    "        given the input tensor and the W_previous\n",
    "        \n",
    "        It is a vector of weight\n",
    "\n",
    "        \"\"\"\n",
    "        return self.sess.run(tf.squeeze(self.action), feed_dict={self.X_t: X_t_, self.W_previous: W_previous_})\n",
    "\n",
    "    \n",
    "    def train(self, X_t_, W_previous_, pf_value_previous_, dailyReturn_t_):\n",
    "        \"\"\"\n",
    "        This function trains the neural network\n",
    "        maximizing the reward \n",
    "        the input is a batch of the differents values\n",
    "        \"\"\"\n",
    "        self.sess.run(self.train_op, feed_dict={self.X_t: X_t_,\n",
    "                                                self.W_previous: W_previous_,\n",
    "                                                self.pf_value_previous: pf_value_previous_,\n",
    "                                                self.dailyReturn_t: dailyReturn_t_})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
