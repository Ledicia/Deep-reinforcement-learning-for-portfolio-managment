{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Trading Agent\n",
    "Class to implement a policy gradient trading agent, who is in charge of finding a policy which maximizes the reward of the portfolio. The agent is a NN whose output is the weight vector $\\vec{w}$ or action.\n",
    "\n",
    "\n",
    "## 1.1 Portolio features:\n",
    "The portfolio features are the tensors that characterize the portfolio:\n",
    "\n",
    "* Relative price tensor $X_t$: Composed by the relative price vectors of the 3 features: \n",
    "\n",
    "$\\text{[closing(0:t-1)/opening price(0:t-1), high(0:t-1))/opening price(0:t-1), low(0:t-1))/opening price(0:t-1)]}$\n",
    "\n",
    "where the shape is $[Bathces, f, m]$, where f are the features, and m the non cash assets.\n",
    "\n",
    "* Relative price vector ($y_t$): Fluctuation of the prices of the assets during the t session ($open(t+1)/open(t)\\approx cose(t)/open(t)$). Shape $[Batches, 1+m]$. It is a rank 2 tensor, and it can be seen as a vector (rank 1 tensor) for each sample/period t in the batch.\n",
    "\n",
    "* W_previous ($w'_{t-1}$): is the portfolio weight vector at the end of the previous trading period. It is given by:\n",
    "$$w'_{t-1} = \\frac{\\vec{y}_{t-1}\\vec{w}_{t-1}}{\\sum_{i=1}^m y_{t-1,i}\\cdot w_{t-1,i}};\\; \\mathrm{Shape}\\; [Batches, 1+m]$$\n",
    "\n",
    "The transaction costs are given by rearranging the assets in the portfolio (buying or selling them) to go from $w'_{t-1}$ to $w_t$ which is the action the agent took for the actual period.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/Trading_Scheme_.PNG\" /></div>\n",
    "\n",
    "The computation of $w'_t$ is done by the environment class, so here is not implemented.\n",
    "\n",
    "* Change on the portfolio value during the session: $P'_t = P_{t}\\cdot \\vec{w}_{t}\\cdot \\vec{y}_t$\n",
    "    - $P_t$ is the value of the portfolio at the beginigin of period t.\n",
    "    - $w_t$ is the portfolio weigth vector at the beginign of period t.\n",
    "    - $P'_t$ is the valie of the portfolio at the end of period t.\n",
    "\n",
    "* Rate of return ($r_t$) or immediate reward: \n",
    "    - Simple return: $\\frac{P'_t-P_{t-1}}{P_{t-1}} = \\mu_t y_t \\cdot w_{t} - 1$\n",
    "    - Continuous compouneded return: $\\log{\\frac{P'_t}{P_{t-1}}}=\\log{\\mu_t y_t \\cdot w_{t}}$. Shape $[Batches, 1]$\n",
    "\n",
    "\n",
    "* Portfolio value vector (pv_vector): Portfolio value for each sample in the batch considering the price evolution\n",
    "    - $[Batch]$ rank 1 tensor (vector):  There is a value per sample/period.\n",
    "    - Portfolio value ($P_f$): is the value of the portfolio anfter $\\Delta t = t_f-bs$ periods (bs = batch size):\n",
    "$$P_{t_f} = P_{t_f-bs} \\exp \\left( \\sum _{t=t_f-bs} ^{t_f + 1} r_t \\right) \\Rightarrow \\frac{P_{t_f}}{P_{t_f-bs}} = \\prod _{t=t_f-bs} ^{t_f+1} \\mu_t \\vec{y}_t \\cdot \\vec{w}_{t}; \\; \\mathrm{Shape}\\; \\mathrm{It\\; is\\; a\\; scalar. Shape\\; []}$$ \n",
    "    - It can also be computed by calculating the transaction costs of changing the portfolio from w_previous = w'_t to action = w_t, obtaining the vector of the asset values in the portfolio V_trans = action*P_prev-(costs,0,...,0), multiply it to consider the fluctuation of prices during the session, suming up V_trans elements for each sample (portfolio value after each period), and then summing up the portfolio value after each period. \n",
    "\n",
    "* Objective function ($R$): is what is going to be maximize. \n",
    "    - Simple return:  \n",
    "    - Log return: It is given by the average of logarithmic cumulated return\n",
    "$$R(s_1, a_1, \\dots, s_{t_f}, a_{t_f}, s_{t_f+1}) = \\frac{1}{t_f}\\log \\left(\\frac{P_f}{P_0}\\right) = \\sum _{t=1}^{t_f+1}\\log (\\mu_t\\vec{y}_t\\cdot \\vec{w}_{t-1}) = \\frac{1}{t_f}\\sum_{t=1}^{t_f+1}r_t; \\; \\mathrm{Shape}\\; [Batches,1]$$\n",
    "    - Sharpe ratio: is the average return earned in excess of the risk-free rate per unit of volatility or total risk.\n",
    "\n",
    "\n",
    "## 1.2 Introducing the cash as an asset\n",
    "\n",
    "The agent introduces the cash as an asset so as to give it more importance when the rest of the assets are devaluing. Since the cash which is not invested has a very little return (the return of the deposit on which it is stored) and it is constant, the profit of not investng cash will be 1+(percentage of interest rate of the deposit/100). Therefore, since the NN receives as an input the closing prices normalized over the highest closing price of the timeseries for each asset, the agent is able to analyze if the asset for the trading period $t$ day is loosing money which respect other days, and, if this loss is worse than the constant return given if the cash was stored in the deposit.\n",
    "\n",
    "Here the cash was not introduced as an asset.\n",
    "\n",
    "## 1.3 Train the NN using batch training: \n",
    "\n",
    "1. A batch starting with period $tb$ $t_0 âˆ’ n_b$ is picked with a geometrically distributed probability (PVM class).\n",
    "2.  It is important that prices inside a batch are in time-order: the slices of the X_t tensor are selected such that X_t[:,:,index:index+n] where the third dimension is the time dimension.\n",
    "3. The for loop which runs through bs index appends into lists the results from each sample in the batch. The lists are converted into arrays that are fed into the NN (and treated as rank 4 tensors):\n",
    "- list_X_t = $(X[:,:,index:index+n], X[:,:,index+1: index+1 + n],\\dots , X[:,:,tb+batch_size: tb+batch\\_size + n])$ where $index = n + tb$\n",
    "- list_W_t = $(W\\_index, W\\_{index+1}, \\dots , W\\_{index+batch\\_size})$  $\\Rightarrow$ Each of this W are calculated at the end of each period (considerinf the evolution of the price during the session)\n",
    "4. train: calls train function defined in the agents class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From C:\\Users\\ledic\\Anaconda3\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ledic\\Anaconda3\\lib\\site-packages\\tflearn\\helpers\\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ledic\\Anaconda3\\lib\\site-packages\\tflearn\\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ledic\\Anaconda3\\lib\\site-packages\\tflearn\\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ledic\\Anaconda3\\lib\\site-packages\\tflearn\\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ledic\\Anaconda3\\lib\\site-packages\\tflearn\\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "class DPG:\n",
    "    \n",
    "    def __init__(self, num_features, num_assets, window_size, device, optimizer, trading_cost, interest_rate,\n",
    "                path_to_save, model_name, LogReturn, load_weights = False, layer_type = 'Conv'):\n",
    "\n",
    "        # parameters\n",
    "        self.trading_cost = trading_cost\n",
    "        self.interest_rate = interest_rate\n",
    "        self.m = num_assets\n",
    "        self.n = window_size\n",
    "        self.num_features = num_features\n",
    "        self.LogReturn = LogReturn\n",
    "        self.layer_type = layer_type\n",
    "        \n",
    "        # Network parameters\n",
    "        self.X_t = tf.placeholder(tf.float32, [None, self.num_features, self.m, self.n]) \n",
    "        self.batch_size = tf.shape(self.X_t)[0]                         # Batch size\n",
    "        self.W_previous = tf.placeholder(tf.float32, [None, self.m])  # w'_{t-1}\n",
    "        self.action = self.build_net()  # Returns the output of imputing X_t and w_previous to the NN\n",
    "        \n",
    "        # Portfolio parameters\n",
    "        self.pf_value_previous = tf.placeholder(tf.float32, [None, 1])               # p'_{t-1} \n",
    "        self.portfolioValuePrevious = tf.squeeze(self.pf_value_previous)             # From [values, 1] to [values]\n",
    "        self.dailyReturn_t = tf.placeholder(tf.float32, [None, self.m])              # y_t = Open(t+1)/Open(t)\n",
    "        constant_return = tf.constant(1+self.interest_rate, shape=[1, 1])            # Interest rate given by cash\n",
    "        self.cash_return = tf.tile(constant_return, tf.stack([self.batch_size, 1]))  # Interest rate is the sae for all samples\n",
    "        self.y_t = tf.concat([self.cash_return, self.dailyReturn_t], axis=1)         # Daily returns considering the cash return\n",
    "                \n",
    "    \n",
    "        # Function to minimize: maximize reward over the batch (min(-r) = max(r))\n",
    "        self.loss_function = self.loss_function()\n",
    "        self.optimizer = optimizer\n",
    "        self.train_op = optimizer.minimize(-self.loss_function)\n",
    "#         self.global_step = tf.Variable(0, trainable=False)\n",
    "#         self.optimize=tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss,global_step=self.global_step)\n",
    "#         self.sess = sess    \n",
    "        tf_config = tf.ConfigProto()\n",
    "        self.sess = tf.Session(config=tf_config)\n",
    "        if device == \"cpu\":\n",
    "            tf_config.gpu_options.per_process_gpu_memory_fraction = 0\n",
    "        else:\n",
    "            tf_config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.path_to_save = path_to_save\n",
    "        self.saver = tf.train.Saver(max_to_keep=10)\n",
    "        \n",
    "        if load_weights:\n",
    "            print(\"Loading Model\")\n",
    "            try:\n",
    "                checkpoint = tf.train.get_checkpoint_state(path_to_save)\n",
    "                print('Saved to:' + path_to_save)\n",
    "                print(checkpoint, checkpoint.model_checkpoint_path)\n",
    "                if checkpoint and checkpoint.model_checkpoint_path:\n",
    "                    tf.reset_default_graph()\n",
    "                    self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n",
    "                    print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
    "                else:\n",
    "                    print(\"Could not find old network weights\")\n",
    "                    self.sess.run(tf.global_variables_initializer())\n",
    "            except:\n",
    "                print(\"Could not find old network weights\")\n",
    "                self.sess.run(tf.global_variables_initializer())\n",
    "        else:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        \n",
    "    def build_net(self):\n",
    "        network = tf.transpose(self.X_t, [0, 2, 3, 1])  # Reshape [Batches, Assets, Periods, Features]\n",
    "        network = tflearn.layers.conv_2d(network, 3,\n",
    "                                         [1, 3],\n",
    "                                         [1, 1, 1, 1],\n",
    "                                         'valid',\n",
    "                                         'relu')\n",
    "        self.first_layer = network\n",
    "        # Second layer \n",
    "        width = network.get_shape()[2]\n",
    "        network = tflearn.layers.conv_2d(network, 48,\n",
    "                                         [1, width],\n",
    "                                         [1, 1],\n",
    "                                         \"valid\",\n",
    "                                         'relu'#)\n",
    "                                         ,regularizer=\"L2\",\n",
    "                                         weight_decay=5e-9)\n",
    "        self.second_layer = network\n",
    "        # Third layer\n",
    "        w_previous = self.W_previous[:,:]\n",
    "        network=tf.concat([network,tf.reshape(w_previous, [-1, self.m, 1, 1])],axis=3)\n",
    "        network = tflearn.layers.conv_2d(network, 1,\n",
    "                                         [1, network.get_shape()[2]],\n",
    "                                         [1, 1],\n",
    "                                         \"valid\",\n",
    "                                         'relu'#)\n",
    "                                         ,regularizer=\"L2\",\n",
    "                                         weight_decay=5e-9)\n",
    "        growth_potential = network[:, :, 0, 0]  # Squeeze diensions [Batchs, assets, 1, 1] = [Batches, Assets]\n",
    "        self.growth_potential = growth_potential\n",
    "        network=tf.layers.flatten(network)\n",
    "        w_init = tf.random_uniform_initializer(-0.005, 0.005)\n",
    "        action = tf.layers.dense(network, self.m, activation=tf.nn.softmax, kernel_initializer=w_init)\n",
    "\n",
    "        return action\n",
    "\n",
    "    # Compute loss funtion\n",
    "    def loss_function(self):\n",
    "        if self.LogReturn:\n",
    "            \n",
    "            # PROFIT VECTOR: P_t/P_{t-1} = exp(r_t) = sum over the assets (action*y_t) \n",
    "            # profit_vector = (y_t1 * w_t1,..., y_tn * w_tn) tn = t1 + batch_size = last period (sample in the batch)\n",
    "            # return_vector = (t_t1, ..., t_tn)\n",
    "            self.profit_vector = tf.reduce_sum(self.action * self.dailyReturn_t, reduction_indices=[1]) * self.compute_mu() \n",
    "            self.return_vector = tf.log(self.profit_vector) \n",
    "\n",
    "            # PROFIT: P(t)/P(t-bs)=exp(sum(_(t-bs)^t) r_t) = (prod(_(t-bs)^t)w_t*y_t) profit obtained after each batch\n",
    "            self.profit = tf.reduce_prod(self.profit_vector)          # Multiplies all the elements of profit_vector\n",
    "            self.mean = tf.reduce_mean(self.return_vector)            # Average daily return (through all the batches)\n",
    "            self.reward = self.mean                                   # Cumulated return (eq 22)\n",
    "            # Risk measure\n",
    "            self.standard_deviation = tf.sqrt(tf.reduce_mean((self.return_vector - self.mean) ** 2))\n",
    "            self.sharpe_ratio = (self.mean) / self.standard_deviation\n",
    "    \n",
    "            loss_function = self.set_loss_function()                  # Loss function to train the NN\n",
    "            \n",
    "\n",
    "        # Simple reward: r_t = (p_t-p_t-1)/p_t-1 = mu_t*y_t*w_t - 1 (w_t = action)\n",
    "        else:   \n",
    "            # Vector of the returns obtained for each period (r_t1, ..., r_tn) such that tn = t1+batch_size\n",
    "            # Return: r_t = (p_t-p_t-1)/p_t-1 => Profit: p_t/p_t-1 = r_t + 1 => p_t = p_t-1(r_1 + 1)\n",
    "            self.return_vector = tf.reduce_sum(self.action * self.y_t, reduction_indices=[1]) * self.compute_mu() - 1\n",
    "            self.profit_vector = 1 + self.return_vector\n",
    "            \n",
    "            self.profit = tf.reduce_prod((1 + self.return_vector))  # P_t/p_t-bs\n",
    "            self.mean = tf.reduce_mean(self.return_vector)          # Average daily return (through all the batches)\n",
    "            self.reward = self.mean                                 # Cumulated return (eq 22)\n",
    "            # self.portfolioValues = self.profit_vector * self.portfolioValuePrevious\n",
    "\n",
    "            # Risk measure\n",
    "            self.standard_deviation = tf.sqrt(tf.reduce_mean((self.return_vector - self.mean) ** 2))\n",
    "            self.sharpe_ratio = (self.mean) / self.standard_deviation\n",
    "            \n",
    "            loss_function =  self.set_loss_function()               # Loss function to train the NN\n",
    "            \n",
    "\n",
    "        return loss_function\n",
    "                \n",
    "    \n",
    "    # Transaction remainder factor \n",
    "    def compute_mu(self):\n",
    "        return 1 - tf.reduce_sum(tf.abs(self.action[:,:]-self.W_previous[:,:]), axis=1) * self.trading_cost # [Batches]\n",
    "    \n",
    "   \n",
    "    # Define the loss function which is going to minimize the agent (so as to maximize the reward)\n",
    "    # Keep in mind that what is going to be minimize is the -loss function (see self.train_op)\n",
    "    def set_loss_function(self):\n",
    "        LAMBDA = 1e-4 \n",
    "        \n",
    "        # Minimizesthe minus cumulated returns (maximizes the cumulated returns)\n",
    "        def loss_function1():\n",
    "            if self.LogReturn:\n",
    "                return self.reward #* 1000000\n",
    "            else: \n",
    "                return self.reward\n",
    "\n",
    "        # Minimizes the minus sharpe ratio   \n",
    "        def loss_function3():\n",
    "            if self.LogReturn:\n",
    "#                 print('Add regularization to avoid actions too big')\n",
    "#                 return self.sharpe_ratio - 0.1*tf.reduce_max(self.action)\n",
    "                return self.sharpe_ratio\n",
    "#                 return self.sharpe_ratio - tf.reduce_mean(tf.reduce_sum(tf.abs(self.action[:, :] - self.W_previous[:,:])\\\n",
    "#                                             *self.trading_cost, reduction_indices=[1])) #- 0.1*tf.reduce_max(self.action)\n",
    "            else: \n",
    "                return self.sharpe_ratio\n",
    "\n",
    "        \n",
    "        loss_function = loss_function1\n",
    "        loss_tensor = loss_function()\n",
    "        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        if regularization_losses:\n",
    "            for regularization_loss in regularization_losses:\n",
    "                loss_tensor += regularization_loss\n",
    "        return loss_tensor\n",
    "\n",
    "    # Compute the agent's action   \n",
    "    def compute_W(self, X_t_, W_previous_):\n",
    "        return self.sess.run(tf.squeeze(self.action), feed_dict={self.X_t: X_t_, self.W_previous: W_previous_})\n",
    "    \n",
    "    # Train the NN maximizing the reward: the input is a batch of the differents values\n",
    "    def train(self, X_t_, W_previous_, pf_value_previous_, dailyReturn_t_):\n",
    "        self.sess.run(self.train_op, feed_dict={self.X_t: X_t_,                             \n",
    "                                                self.W_previous: W_previous_,\n",
    "                                                self.pf_value_previous: pf_value_previous_,\n",
    "                                                self.dailyReturn_t: dailyReturn_t_})\n",
    "        \n",
    "    # Save model parameters\n",
    "    def save_model(self):\n",
    "        if not os.path.exists(self.path_to_save):\n",
    "            os.makedirs(self.path_to_save)\n",
    "        self.saver.save(self.sess, self.path_to_save + self.model_name)\n",
    "        \n",
    "    # Getters of interesting variables: \n",
    "    def get_sharpe_ratio(self,  X_t_, W_previous_, pf_value_previous_, dailyReturn_t_):\n",
    "        return self.sess.run(self.sharpe_ratio, feed_dict={self.X_t: X_t_,                             \n",
    "                                                self.W_previous: W_previous_,\n",
    "                                                self.pf_value_previous: pf_value_previous_,\n",
    "                                                self.dailyReturn_t: dailyReturn_t_})\n",
    "    def get_average_daily_return(self,  X_t_, W_previous_, pf_value_previous_, dailyReturn_t_):\n",
    "        return self.sess.run(self.mean, feed_dict={self.X_t: X_t_,                             \n",
    "                                                self.W_previous: W_previous_,\n",
    "                                                self.pf_value_previous: pf_value_previous_,\n",
    "                                                self.dailyReturn_t: dailyReturn_t_})\n",
    "    def get_profit(self,  X_t_, W_previous_, pf_value_previous_, dailyReturn_t_):\n",
    "        return self.sess.run(self.profit, feed_dict={self.X_t: X_t_,                             \n",
    "                                                self.W_previous: W_previous_,\n",
    "                                                self.pf_value_previous: pf_value_previous_,\n",
    "                                                self.dailyReturn_t: dailyReturn_t_})\n",
    "    def get_loss_function(self,  X_t_, W_previous_, pf_value_previous_, dailyReturn_t_):\n",
    "        return self.sess.run(self.loss_function, feed_dict={self.X_t: X_t_,                             \n",
    "                                                self.W_previous: W_previous_,\n",
    "                                                self.pf_value_previous: pf_value_previous_,\n",
    "                                                self.dailyReturn_t: dailyReturn_t_})\n",
    "    \n",
    "    def get_w_evol(self,  X_t_, W_previous_, dailyReturn_t_):\n",
    "        return self.sess.run(self.w_evol, feed_dict={self.X_t: X_t_,                             \n",
    "                                                self.W_previous: W_previous_,\n",
    "                                                self.dailyReturn_t: dailyReturn_t_})\n",
    "    def get_first_layer(self, X_t_, W_previous_):\n",
    "        return self.sess.run(self.first_layer, feed_dict={self.X_t: X_t_, self.W_previous: W_previous_})\n",
    "    \n",
    "    def get_second_layer(self, X_t_, W_previous_):\n",
    "        return self.sess.run(self.second_layer, feed_dict={self.X_t: X_t_, self.W_previous: W_previous_})\n",
    "    \n",
    "    def get_growth_potential(self, X_t_, W_previous_):\n",
    "        return self.sess.run(self.growth_potential, feed_dict={self.X_t: X_t_, self.W_previous: W_previous_})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# tf.reset_default_graph()\n",
    "# sess = tf.Session()\n",
    "# optimizer = tf.train.AdamOptimizer(9e-2)\n",
    "# # Initialize network agents\n",
    "# simple_agent = DPG(4, 5, 10, sess, optimizer, 0.0025, 0.005, LogReturn = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
